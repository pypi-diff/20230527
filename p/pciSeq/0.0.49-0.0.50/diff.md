# Comparing `tmp/pciSeq-0.0.49-py3-none-any.whl.zip` & `tmp/pciSeq-0.0.50-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,69 +1,69 @@
-Zip file size: 352378 bytes, number of entries: 67
--rw-rw-rw-  2.0 fat     2086 b- defN 22-Nov-15 19:09 pciSeq/__init__.py
--rw-rw-rw-  2.0 fat    10388 b- defN 23-Mar-10 15:01 pciSeq/app.py
--rw-rw-rw-  2.0 fat     3333 b- defN 23-Mar-10 14:58 pciSeq/config.py
+Zip file size: 353530 bytes, number of entries: 67
+-rw-rw-rw-  2.0 fat     2086 b- defN 23-Apr-26 22:13 pciSeq/__init__.py
+-rw-rw-rw-  2.0 fat     9326 b- defN 23-May-27 20:49 pciSeq/app.py
+-rw-rw-rw-  2.0 fat     3333 b- defN 23-May-27 19:24 pciSeq/config.py
 -rw-rw-rw-  2.0 fat     3678 b- defN 22-Apr-07 23:15 pciSeq/make_plot.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Jan-19 17:53 pciSeq/src/__init__.py
--rw-rw-rw-  2.0 fat       22 b- defN 23-Mar-10 15:01 pciSeq/src/_version.py
+-rw-rw-rw-  2.0 fat       22 b- defN 23-May-27 20:49 pciSeq/src/_version.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Jan-19 17:53 pciSeq/src/cell_call/__init__.py
--rw-rw-rw-  2.0 fat    16311 b- defN 23-Mar-04 21:22 pciSeq/src/cell_call/datatypes.py
--rw-rw-rw-  2.0 fat     2995 b- defN 23-Mar-07 23:23 pciSeq/src/cell_call/log_config.py
--rw-rw-rw-  2.0 fat    10436 b- defN 23-Mar-10 01:09 pciSeq/src/cell_call/main.py
--rw-rw-rw-  2.0 fat     3110 b- defN 23-Mar-04 21:22 pciSeq/src/cell_call/summary.py
--rw-rw-rw-  2.0 fat     8939 b- defN 23-Mar-10 01:09 pciSeq/src/cell_call/utils.py
+-rw-rw-rw-  2.0 fat    20581 b- defN 23-May-27 20:49 pciSeq/src/cell_call/datatypes.py
+-rw-rw-rw-  2.0 fat     2995 b- defN 23-Apr-26 22:13 pciSeq/src/cell_call/log_config.py
+-rw-rw-rw-  2.0 fat    13927 b- defN 23-May-27 20:49 pciSeq/src/cell_call/main.py
+-rw-rw-rw-  2.0 fat     3104 b- defN 23-May-27 20:49 pciSeq/src/cell_call/summary.py
+-rw-rw-rw-  2.0 fat     8937 b- defN 23-May-27 20:49 pciSeq/src/cell_call/utils.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Jan-19 17:53 pciSeq/src/preprocess/__init__.py
--rw-rw-rw-  2.0 fat    10926 b- defN 22-Oct-09 21:52 pciSeq/src/preprocess/cell_borders.py
--rw-rw-rw-  2.0 fat     6973 b- defN 22-Oct-09 21:52 pciSeq/src/preprocess/segmentation.py
--rw-rw-rw-  2.0 fat     4099 b- defN 23-Mar-04 21:22 pciSeq/src/preprocess/spot_labels.py
--rw-rw-rw-  2.0 fat       88 b- defN 23-Mar-10 01:09 pciSeq/src/preprocess/utils.py
+-rw-rw-rw-  2.0 fat    11253 b- defN 23-May-27 20:49 pciSeq/src/preprocess/cell_borders.py
+-rw-rw-rw-  2.0 fat     6973 b- defN 23-Apr-26 22:13 pciSeq/src/preprocess/segmentation.py
+-rw-rw-rw-  2.0 fat     4090 b- defN 23-May-27 20:49 pciSeq/src/preprocess/spot_labels.py
+-rw-rw-rw-  2.0 fat       88 b- defN 23-May-27 09:58 pciSeq/src/preprocess/utils.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Feb-20 14:12 pciSeq/src/viewer/__init__.py
--rw-rw-rw-  2.0 fat     2442 b- defN 23-Mar-10 01:09 pciSeq/src/viewer/run_flask.py
--rw-rw-rw-  2.0 fat     4511 b- defN 22-Oct-09 21:52 pciSeq/src/viewer/stage_image.py
--rw-rw-rw-  2.0 fat     8464 b- defN 23-Mar-04 21:22 pciSeq/src/viewer/utils.py
--rw-rw-rw-  2.0 fat    10895 b- defN 23-Mar-10 01:09 pciSeq/static/2D/index.html
--rw-rw-rw-  2.0 fat     6506 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/genes_datatable.html
--rw-rw-rw-  2.0 fat     9622 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/css/index.css
--rw-rw-rw-  2.0 fat     3135 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/css/progress.css
--rw-rw-rw-  2.0 fat     7435 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/classConfig.js
--rw-rw-rw-  2.0 fat     1211 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/config.js
--rw-rw-rw-  2.0 fat     5589 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/customControl.js
--rw-rw-rw-  2.0 fat    22840 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/dapi.js
--rw-rw-rw-  2.0 fat     5984 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/dataLoader.js
--rw-rw-rw-  2.0 fat    17232 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/donut.js
--rw-rw-rw-  2.0 fat     4106 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/dt.js
--rw-rw-rw-  2.0 fat     9616 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/glyphConfig.js
--rw-rw-rw-  2.0 fat     4389 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/glyphPaths.js
--rw-rw-rw-  2.0 fat     7224 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/glyphs.js
--rw-rw-rw-  2.0 fat    12281 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/index.js
--rw-rw-rw-  2.0 fat     1073 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/progress.js
--rw-rw-rw-  2.0 fat     6367 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/stage_cells.js
--rw-rw-rw-  2.0 fat    12064 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/stage_glyphs.js
--rw-rw-rw-  2.0 fat     9540 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/stage_markers.js
--rw-rw-rw-  2.0 fat     7533 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/stage_markers_patched.js
--rw-rw-rw-  2.0 fat    15523 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/stage_polygons.js
--rw-rw-rw-  2.0 fat     4471 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/streaming-tsv-parser.js
--rw-rw-rw-  2.0 fat     4692 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/viewerUtils.js
--rw-rw-rw-  2.0 fat      995 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/css/L.Control.Layers.Tree.css
--rw-rw-rw-  2.0 fat      421 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/css/Leaflet.Coordinates-0.1.3.css
--rw-rw-rw-  2.0 fat   122544 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/css/bootstrap.min.css
--rw-rw-rw-  2.0 fat     8669 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/css/index.css
--rw-rw-rw-  2.0 fat     1228 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/css/keen-dashboards.css
--rw-rw-rw-  2.0 fat     5245 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/css/screen.css
--rw-rw-rw-  2.0 fat    26652 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/js/L.Control.Layers.Tree.js
--rw-rw-rw-  2.0 fat     8813 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/js/Leaflet.Coordinates-0.1.3.src.js
--rw-rw-rw-  2.0 fat    31824 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/js/bootstrap.min.js
--rw-rw-rw-  2.0 fat    42707 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/js/keen.min.js
--rw-rw-rw-  2.0 fat    15374 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/js/leaflet-pip.js
--rw-rw-rw-  2.0 fat     5927 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/js/leaflet.textpath.js
--rw-rw-rw-  2.0 fat      265 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/js/preloader.js
--rw-rw-rw-  2.0 fat     9097 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/js/pixiOverlay/MarkerContainer.js
--rw-rw-rw-  2.0 fat     4434 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/js/pixiOverlay/bezier-easing.js
--rw-rw-rw-  2.0 fat   613808 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/js/pixiOverlay/example.min.js
--rw-rw-rw-  2.0 fat    91874 b- defN 23-Mar-10 01:09 pciSeq/static/2D/viewer/js/lib/js/pixiOverlay/tools.min.js
--rw-rw-rw-  2.0 fat     1091 b- defN 23-Mar-10 15:02 pciSeq-0.0.49.dist-info/LICENCE
--rw-rw-rw-  2.0 fat     3749 b- defN 23-Mar-10 15:02 pciSeq-0.0.49.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Mar-10 15:02 pciSeq-0.0.49.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       48 b- defN 23-Mar-10 15:02 pciSeq-0.0.49.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat        7 b- defN 23-Mar-10 15:02 pciSeq-0.0.49.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     6384 b- defN 23-Mar-10 15:02 pciSeq-0.0.49.dist-info/RECORD
-67 files, 1289377 bytes uncompressed, 342012 bytes compressed:  73.5%
+-rw-rw-rw-  2.0 fat     2256 b- defN 23-May-27 09:58 pciSeq/src/viewer/run_flask.py
+-rw-rw-rw-  2.0 fat     4511 b- defN 23-Apr-26 22:13 pciSeq/src/viewer/stage_image.py
+-rw-rw-rw-  2.0 fat     7638 b- defN 23-May-27 20:49 pciSeq/src/viewer/utils.py
+-rw-rw-rw-  2.0 fat    10895 b- defN 23-May-27 09:58 pciSeq/static/2D/index.html
+-rw-rw-rw-  2.0 fat     6506 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/genes_datatable.html
+-rw-rw-rw-  2.0 fat     9622 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/css/index.css
+-rw-rw-rw-  2.0 fat     3135 b- defN 23-May-27 09:58 pciSeq/static/2D/viewer/css/progress.css
+-rw-rw-rw-  2.0 fat     7435 b- defN 23-May-27 20:48 pciSeq/static/2D/viewer/js/classConfig.js
+-rw-rw-rw-  2.0 fat     1211 b- defN 23-May-27 09:58 pciSeq/static/2D/viewer/js/config.js
+-rw-rw-rw-  2.0 fat     5589 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/customControl.js
+-rw-rw-rw-  2.0 fat    22840 b- defN 23-May-27 09:58 pciSeq/static/2D/viewer/js/dapi.js
+-rw-rw-rw-  2.0 fat     5984 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/dataLoader.js
+-rw-rw-rw-  2.0 fat    17232 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/donut.js
+-rw-rw-rw-  2.0 fat     4106 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/dt.js
+-rw-rw-rw-  2.0 fat     9835 b- defN 23-May-27 09:58 pciSeq/static/2D/viewer/js/glyphConfig.js
+-rw-rw-rw-  2.0 fat     4389 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/glyphPaths.js
+-rw-rw-rw-  2.0 fat     7224 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/glyphs.js
+-rw-rw-rw-  2.0 fat    12281 b- defN 23-May-27 09:58 pciSeq/static/2D/viewer/js/index.js
+-rw-rw-rw-  2.0 fat     1073 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/progress.js
+-rw-rw-rw-  2.0 fat     6367 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/stage_cells.js
+-rw-rw-rw-  2.0 fat    12064 b- defN 23-May-27 09:58 pciSeq/static/2D/viewer/js/stage_glyphs.js
+-rw-rw-rw-  2.0 fat     9540 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/stage_markers.js
+-rw-rw-rw-  2.0 fat     7150 b- defN 23-May-27 09:58 pciSeq/static/2D/viewer/js/stage_markers_patched.js
+-rw-rw-rw-  2.0 fat    15523 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/stage_polygons.js
+-rw-rw-rw-  2.0 fat     4471 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/streaming-tsv-parser.js
+-rw-rw-rw-  2.0 fat     4692 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/viewerUtils.js
+-rw-rw-rw-  2.0 fat      995 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/css/L.Control.Layers.Tree.css
+-rw-rw-rw-  2.0 fat      421 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/css/Leaflet.Coordinates-0.1.3.css
+-rw-rw-rw-  2.0 fat   122544 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/css/bootstrap.min.css
+-rw-rw-rw-  2.0 fat     8669 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/css/index.css
+-rw-rw-rw-  2.0 fat     1228 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/css/keen-dashboards.css
+-rw-rw-rw-  2.0 fat     5245 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/css/screen.css
+-rw-rw-rw-  2.0 fat    26652 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/js/L.Control.Layers.Tree.js
+-rw-rw-rw-  2.0 fat     8813 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/js/Leaflet.Coordinates-0.1.3.src.js
+-rw-rw-rw-  2.0 fat    31824 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/js/bootstrap.min.js
+-rw-rw-rw-  2.0 fat    42707 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/js/keen.min.js
+-rw-rw-rw-  2.0 fat    15374 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/js/leaflet-pip.js
+-rw-rw-rw-  2.0 fat     5927 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/js/leaflet.textpath.js
+-rw-rw-rw-  2.0 fat      265 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/js/preloader.js
+-rw-rw-rw-  2.0 fat     9097 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/js/pixiOverlay/MarkerContainer.js
+-rw-rw-rw-  2.0 fat     4434 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/js/pixiOverlay/bezier-easing.js
+-rw-rw-rw-  2.0 fat   613808 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/js/pixiOverlay/example.min.js
+-rw-rw-rw-  2.0 fat    91874 b- defN 23-Apr-26 22:13 pciSeq/static/2D/viewer/js/lib/js/pixiOverlay/tools.min.js
+-rw-rw-rw-  2.0 fat     1091 b- defN 23-May-27 20:50 pciSeq-0.0.50.dist-info/LICENCE
+-rw-rw-rw-  2.0 fat     3697 b- defN 23-May-27 20:50 pciSeq-0.0.50.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-May-27 20:50 pciSeq-0.0.50.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       48 b- defN 23-May-27 20:50 pciSeq-0.0.50.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat        7 b- defN 23-May-27 20:50 pciSeq-0.0.50.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     6383 b- defN 23-May-27 20:50 pciSeq-0.0.50.dist-info/RECORD
+67 files, 1295157 bytes uncompressed, 343164 bytes compressed:  73.5%
```

## zipnote {}

```diff
@@ -177,26 +177,26 @@
 
 Filename: pciSeq/static/2D/viewer/js/lib/js/pixiOverlay/example.min.js
 Comment: 
 
 Filename: pciSeq/static/2D/viewer/js/lib/js/pixiOverlay/tools.min.js
 Comment: 
 
-Filename: pciSeq-0.0.49.dist-info/LICENCE
+Filename: pciSeq-0.0.50.dist-info/LICENCE
 Comment: 
 
-Filename: pciSeq-0.0.49.dist-info/METADATA
+Filename: pciSeq-0.0.50.dist-info/METADATA
 Comment: 
 
-Filename: pciSeq-0.0.49.dist-info/WHEEL
+Filename: pciSeq-0.0.50.dist-info/WHEEL
 Comment: 
 
-Filename: pciSeq-0.0.49.dist-info/entry_points.txt
+Filename: pciSeq-0.0.50.dist-info/entry_points.txt
 Comment: 
 
-Filename: pciSeq-0.0.49.dist-info/top_level.txt
+Filename: pciSeq-0.0.50.dist-info/top_level.txt
 Comment: 
 
-Filename: pciSeq-0.0.49.dist-info/RECORD
+Filename: pciSeq-0.0.50.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pciSeq/app.py

```diff
@@ -1,61 +1,61 @@
 import os
 import pandas as pd
 import numpy as np
-import subprocess
-from email.parser import BytesHeaderParser
-import shutil
-import json
 import tempfile
 import pickle
 from typing import Tuple
 from scipy.sparse import coo_matrix, save_npz, load_npz
 from pciSeq.src.cell_call.main import VarBayes
-from pciSeq.src.preprocess.spot_labels import stage_data
 from pciSeq.src.cell_call.utils import get_out_dir
+from pciSeq.src.preprocess.spot_labels import stage_data
 from pciSeq.src.preprocess.utils import get_img_shape
 from pciSeq.src.viewer.run_flask import flask_app_start
+from pciSeq.src.viewer.utils import copy_viewer_code, make_config_js, make_classConfig_js
 from pciSeq import config
 from pciSeq.src.cell_call.log_config import attach_to_log, logger
 
 ROOT_DIR = os.path.dirname(os.path.realpath(__file__))
 
 
-def fit(iss_spots: pd.DataFrame, coo: coo_matrix, scRNAseq: pd.DataFrame, opts: dict = None) -> Tuple[pd.DataFrame, pd.DataFrame]:
+def fit(*args, **kwargs) -> Tuple[pd.DataFrame, pd.DataFrame]:
     """
     Main entry point for pciSeq.
 
     Parameters
     ----------
-    iss_spots : pandas.DataFrame
+    **spots : pandas.DataFrame
         Index:
             RangeIndex
         Columns:
             Name: Gene, dtype: string, The gene name
             Name: x, dtype: int64, X-axis coordinate of the spot
             Name: y, dtype: int64, Y-axis coordinate of the spot
 
-    coo : scipy.sparse.coo_matrix
+    **coo : scipy.sparse.coo_matrix
         A label image array as a coo_matrix datatype. The label denote
         which cell the corresponding pixel 'belongs' to. If label is
         zero, the pixel is on the background
 
-    scRNAseq : pandas.DataFrame
+    **scRNAseq : pandas.DataFrame (Optional)
         Index:
             The gene name
         Columns:
             The column headers are the cell classes and the data are uint32
 
-    opts : dictionary (Optional)
+    **opts : dictionary (Optional)
         A dictionary to pass-in user-defined hyperparameter values. They override the default
         values as these are set by the config.py file. For example to exclude genes Npy and
         Vip you can define opts as:
             opts = {'exclude_genes': ['Npy', 'Vip']}
         and pass that dict to the fit function as the last argument
 
+    *args: If 'spots' and 'coo' are not passed-in as keyword arguments then they should be provided
+    as first and second positional arguments
+
     Returns
     ------
     cellData : pandas.DataFrame
         Index:
             RangeIndex
         Columns:
             Name: Cell_Num, dtype: int64, The label of the cell
@@ -75,70 +75,114 @@
             Name: x, dtype: int64, X-axis coordinate of the spot
             Name: y, dtype: int64, Y-axis coordinate of the spot
             Name: neighbour, dtype: int64, the label of the cell which is more likely to 'raise' the spot. If zero then the spot is a misread.
             Name: neighbour_array, dtype: Object, array-like with the labels of the 4 nearest cell. The last is always the background and has label=0
             Name: neighbour_prob, dtype: Object, array-like with the prob the corresponding cell from neighbour_array has risen the spot.
     """
 
+    if not {'spots', 'coo'}.issubset(set(kwargs)):
+        try:
+            assert len(args) == 2, 'Need to provide the spots and the coo matrix as the first ' \
+                                   'and second args to the fit() method '
+            kwargs['spots'] = args[0]
+            kwargs['coo'] = args[1]
+        except Exception as err:
+            raise
+
+    try:
+        spots = kwargs['spots']
+        coo = kwargs['coo']
+    except Exception as err:
+        raise
+
+    try:
+        scRNAseq = kwargs['scRNAseq']
+    except KeyError:
+        scRNAseq = None
+    except Exception as err:
+        raise
+
+    try:
+        opts = kwargs['opts']
+    except KeyError:
+        opts = None
+    except Exception as err:
+        raise
+
     # 1. get the hyperparameters
     cfg = init(opts)
 
     # 2. validate inputs
-    validate(iss_spots, scRNAseq)
+    validate(spots, coo, scRNAseq)
 
     # 3. prepare the data
     logger.info(' Preprocessing data')
-    _cells, cellBoundaries, _spots = stage_data(iss_spots, coo)
+    _cells, cellBoundaries, _spots = stage_data(spots, coo)
 
     # 4. cell typing
     cellData, geneData, varBayes = cell_type(_cells, _spots, scRNAseq, cfg)
 
     # 5. save to the filesystem
     if (cfg['save_data'] and varBayes.has_converged) or cfg['launch_viewer']:
-        write_data(cellData, geneData, cellBoundaries, varBayes, cfg['output_path'])
+        write_data(cellData, geneData, cellBoundaries, varBayes, cfg)
 
+    # 6. do the viewer if needed
     if cfg['launch_viewer']:
         [h, w] = get_img_shape(coo)
-        dst = copy_viewer_code(cfg)
+        dst = get_out_dir(cfg['output_path'])
+        copy_viewer_code(cfg, dst)
         make_config_js(dst, w, h)
+        if scRNAseq is None:
+            label_list = [d[:] for d in cellData.ClassName.values]
+            labels = [item for sublist in label_list for item in sublist]
+            labels = sorted(set(labels))
+            if 'Zero' in labels:
+                # remove Zero because it will be appended later on by
+                # the make_classConfig_js script
+                labels.remove('Zero')
+            make_classConfig_js(labels, dst)
         flask_app_start(dst)
 
     logger.info(' Done')
     return cellData, geneData
 
 
 def cell_type(_cells, _spots, scRNAseq, ini):
     varBayes = VarBayes(_cells, _spots, scRNAseq, ini)
 
     logger.info(' Start cell typing')
     cellData, geneData = varBayes.run()
     return cellData, geneData, varBayes
 
 
-def write_data(cellData, geneData, cellBoundaries, varBayes, path):
-    if path[0] == 'default':
-        out_dir = os.path.join(tempfile.gettempdir(), 'pciSeq', 'data')
-    else:
-        out_dir = path[0]
+def write_data(cellData, geneData, cellBoundaries, varBayes, cfg):
+    dst = get_out_dir(cfg['output_path'])
+    out_dir = os.path.join(dst, 'data')
     if not os.path.exists(out_dir):
         os.makedirs(out_dir)
 
     cellData.to_csv(os.path.join(out_dir, 'cellData.tsv'), sep='\t', index=False)
     logger.info(' Saved at %s' % (os.path.join(out_dir, 'cellData.tsv')))
 
     geneData.to_csv(os.path.join(out_dir, 'geneData.tsv'), sep='\t', index=False)
     logger.info(' Saved at %s' % (os.path.join(out_dir, 'geneData.tsv')))
 
     cellBoundaries.to_csv(os.path.join(out_dir, 'cellBoundaries.tsv'), sep='\t', index=False)
     logger.info(' Saved at %s' % (os.path.join(out_dir, 'cellBoundaries.tsv')))
 
-    ## commenting this as there is no need to save the pickle file at the moment
-    # with open(os.path.join(out_dir, 'pciSeq.pickle'), 'wb') as outf:
-    #     pickle.dump(varBayes, outf)
-    #     logger.info(' Saved at %s' % os.path.join(out_dir, 'pciSeq.pickle'))
+    serialise(varBayes, os.path.join(out_dir, 'debug'))
+
+
+def serialise(varBayes, debug_dir):
+    if not os.path.exists(debug_dir):
+        os.makedirs(debug_dir)
+    pickle_dst = os.path.join(debug_dir, 'pciSeq.pickle')
+    with open(pickle_dst, 'wb') as outf:
+        pickle.dump(varBayes, outf)
+        logger.info(' Saved at %s' % pickle_dst)
 
 
 def init(opts):
     """
     Reads the opts dict and if not None, it will override the default parameter value by
     the value that the dictionary key points to.
     If opts is None, then the default values as these specified in the config.py file
@@ -157,72 +201,22 @@
             else:
                 raise TypeError("Only integers, floats and lists are allowed")
             cfg[item[0]] = val
             logger.info(' %s is set to %s' % (item[0], cfg[item[0]]))
     return cfg
 
 
-def validate(spots, sc):
+def validate(spots, coo, sc):
     assert isinstance(spots, pd.DataFrame) and set(spots.columns) == {'Gene', 'x', 'y'}, \
         "Spots should be passed-in to the fit() method as a dataframe with columns ['Gene', 'x', 'y']"
 
-    assert isinstance(sc, pd.DataFrame), "Single cell data should be passed-in to the fit() method as a dataframe"
-
+    assert isinstance(coo, coo_matrix), 'The segmentation masks should be passed-in as a coo_matrix'
 
-def make_config_base(dst):
-    cellData_tsv = os.path.join(dst, 'data', 'cellData.tsv')
-    geneData_tsv = os.path.join(dst, 'data', 'geneData.tsv')
-
-    cellData_dict = {"mediaLink": "../../data/cellData.tsv", "size": str(os.path.getsize(cellData_tsv))}
-    geneData_dict = {"mediaLink": "../../data/geneData.tsv", "size": str(os.path.getsize(geneData_tsv))}
-
-    return {
-        'cellData': cellData_dict,
-        'geneData': geneData_dict,
-    }
-
-
-def make_config_js(dst, w, h):
-    appDict = make_config_base(dst)
-    cellBoundaries_tsv = os.path.join(dst, 'data', 'cellBoundaries.tsv')
-    cellBoundaries_dict = {"mediaLink": "../../data/cellBoundaries.tsv", "size": str(os.path.getsize(cellBoundaries_tsv))}
-    roi_dict = {"x0": 0, "x1": w, "y0": 0, "y1": h}
-    appDict['cellBoundaries'] = cellBoundaries_dict
-    appDict['roi'] = roi_dict
-    appDict['zoomLevels'] = 10
-    appDict['tiles'] = "https://storage.googleapis.com/ca1-data/img/262144px/{z}/{y}/{x}.jpg"
-
-    config_str = "// NOTES: \n" \
-                 "// 1. paths are with respect to the location of 'streaming-tsv-parser.js \n" \
-                 "// 2. roi is the image size in pixels. Leave x0 and y0 at zero and set x1 to the width and y1 to the height \n" \
-                 "// 3. tiles should point to the folder that keeps your pyramid of tiles. If you do not have that just \n" \
-                 "//    change the link to a blind one (change the jpg extension for example). The viewer should work \n" \
-                 "//    without the dapi background though \n" \
-                 "// 4. size is the tsv size in bytes. I use os.path.getsize() to get it. Not crucial if you \n" \
-                 "//    dont get it right, ie the full tsv will still be parsed despite this being wrong. It \n" \
-                 "//    is used by the loading page piecharts to calc how far we are \n" \
-                 "// 5. Leave zoomLevels to 10 \n" \
-                 " function config() { return %s }" % json.dumps(appDict)
-    config = os.path.join(dst, 'viewer', 'js', 'config.js')
-    with open(config, 'w') as data:
-        data.write(str(config_str))
-    logger.info(' viewer config saved at %s' % config)
-
-
-def copy_viewer_code(cfg):
-    p = subprocess.run(['pip', 'show', 'pciSeq'], stdout=subprocess.PIPE)
-    h = BytesHeaderParser().parsebytes(p.stdout)
-    pciSeq_dir = os.path.join(h['Location'], 'pciSeq')
-    dim = '2D'
-    src = os.path.join(pciSeq_dir, 'static', dim)
-    dst = get_out_dir(cfg['output_path'], '')
-
-    shutil.copytree(src, dst, dirs_exist_ok=True)
-    logger.info(' viewer code (%s) copied from %s to %s' % (dim, src, dst))
-    return dst
+    if sc is not None:
+        assert isinstance(sc, pd.DataFrame), "Single cell data should be passed-in to the fit() method as a dataframe"
 
 
 if __name__ == "__main__":
     # set up the logger
     attach_to_log()
 
     # read some demo data
@@ -232,9 +226,10 @@
     _scRNAseq = pd.read_csv(os.path.join(ROOT_DIR, 'data', 'mouse', 'ca1', 'scRNA', 'scRNAseq.csv.gz'),
                             header=None, index_col=0, compression='gzip', dtype=object)
     _scRNAseq = _scRNAseq.rename(columns=_scRNAseq.iloc[0], copy=False).iloc[1:]
     _scRNAseq = _scRNAseq.astype(float).astype(np.uint32)
 
     # main task
     # _opts = {'max_iter': 10}
-    fit(_iss_spots, _coo, _scRNAseq, {'save_data': True})
+    fit(spots=_iss_spots, coo=_coo, scRNAseq=None, opts={'save_data': True,
+                                                         'launch_viewer': True,})
```

## pciSeq/src/_version.py

```diff
@@ -1 +1 @@
-__version__ = '0.0.49'
+__version__ = '0.0.50'
```

## pciSeq/src/cell_call/datatypes.py

```diff
@@ -7,27 +7,27 @@
 from pciSeq.src.cell_call.log_config import logger
 
 
 class Cells(object):
     # Get rid of the properties where not necessary!!
     def __init__(self, _cells_df, config):
         self.config = config
-        self.cell_props, self.mcr = self.read_image_objects(_cells_df, config)
-        self.nC = len(self.cell_props['cell_label'])
+        self.ini_cell_props, self.mcr = self.read_image_objects(_cells_df, config)
+        self.nC = len(self.ini_cell_props['cell_label'])
         self.classProb = None
         self.class_names = None
         self._cov = self.ini_cov()
+        self._centroid = self.ini_centroids()
         self._gene_counts = None
         self._background_counts = None
 
     # -------- PROPERTIES -------- #
     @property
     def yx_coords(self):
-        coords = [d for d in zip(self.cell_props['y'], self.cell_props['x']) if not np.isnan(d).any()]
-        return np.array(coords)
+        return self.centroid[['y', 'x']].values
 
     @property
     def geneCount(self):
         return self._gene_counts
 
     @geneCount.setter
     def geneCount(self, val):
@@ -44,22 +44,42 @@
         self._background_counts = val
 
     @property
     def total_counts(self):
         # tc = self.geneCount.sum(axis=1)
         return self.geneCount.sum(axis=1)
 
+    @property
+    def centroid(self):
+        # lst = list(zip(*[self._centroid['x'], self._centroid['y']]))
+        return self._centroid.copy()
+
+    @centroid.setter
+    def centroid(self, df):
+        assert isinstance(df, pd.DataFrame), 'Input should be a dataframe'
+        assert set(df.columns.values) == {'x', 'y'}, 'Dataframe columns should be ''x'', ''y'' '
+        df.index.name = 'cell_label'
+        self._centroid = df.copy()
+
     # -------- METHODS -------- #
+    def ini_centroids(self):
+        d = {
+            'x': self.ini_cell_props['x0'],
+            'y': self.ini_cell_props['y0'],
+        }
+        df = pd.DataFrame(d)
+        return df.copy()
+
     def ini_cov(self):
         mcr = self.dapi_mean_cell_radius()
         cov = mcr * mcr * np.eye(2, 2)
         return np.tile(cov, (self.nC, 1, 1))
 
     def dapi_mean_cell_radius(self):
-        return np.nanmean(np.sqrt(self.cell_props['area'] / np.pi)) * 0.5
+        return np.nanmean(np.sqrt(self.ini_cell_props['area'] / np.pi)) * 0.5
 
     def nn(self):
         n = self.config['nNeighbors'] + 1
         # for each spot find the closest cell (in fact the top nN-closest cells...)
         nbrs = NearestNeighbors(n_neighbors=n, algorithm='ball_tree').fit(self.yx_coords)
         return nbrs
 
@@ -91,16 +111,16 @@
 
         out = {}
         out['area_factor'] = CellAreaFactor
         # out['area_factor'] = np.ones(CellAreaFactor.shape)
         # logger.info('Overriden CellAreaFactor = 1')
         out['rel_radius'] = relCellRadius
         out['area'] = np.append(np.nan, img_obj.area)
-        out['x'] = np.append(-sys.maxsize, img_obj.x.values)
-        out['y'] = np.append(-sys.maxsize, img_obj.y.values)
+        out['x0'] = np.append(-sys.maxsize, img_obj.x0.values)
+        out['y0'] = np.append(-sys.maxsize, img_obj.y0.values)
         out['cell_label'] = np.append(0, img_obj.label.values)
         # First cell is a dummy cell, a super neighbour (ie always a neighbour to any given cell)
         # and will be used to get all the misreads. It was given the label=0 and some very small
         # negative coords
 
         return out, meanCellRadius
 
@@ -109,18 +129,14 @@
     def __init__(self, spots):
         self.gene_panel = np.unique(spots.data.gene_name.values)
         self._eta_bar = None
         self._logeta_bar = None
         self.nG = len(self.gene_panel)
 
     @property
-    def eta(self):
-        raise Exception
-
-    @property
     def eta_bar(self):
         return self._eta_bar
 
     @property
     def logeta_bar(self):
         return self._logeta_bar
 
@@ -140,20 +156,30 @@
 class Spots(object):
     def __init__(self, spots_df, config):
         self._parent_cell_prob = None
         self._parent_cell_id = None
         self.config = config
         self.data = self.read(spots_df)
         self.nS = self.data.shape[0]
-        self.call = None
         self.unique_gene_names = None
         self._gamma_bar = None
         self._log_gamma_bar = None
         [_, self.gene_id, self.counts_per_gene] = np.unique(self.data.gene_name.values, return_inverse=True, return_counts=True)
 
+    def __getstate__(self):
+        # set here attributes to be excluded from serialisation (pickling)
+        # It makes the pickle filesize smaller but maybe this will have to
+        # change in the future.
+        # These two attributes take up a lot of space on the disk:
+        # _gamma_bar and _log_gamma_bar
+        # FYI: https://realpython.com/python-pickle-module/
+        attributes = self.__dict__.copy()
+        del attributes['_gamma_bar']
+        del attributes['_log_gamma_bar']
+
     # -------- PROPERTIES -------- #
     @property
     def gamma_bar(self):
         return self._gamma_bar.astype(self.config['dtype'])
 
     @gamma_bar.setter
     def gamma_bar(self, val):
@@ -228,15 +254,15 @@
         pSpotNeighb[neighbors == SpotInCell.values[:, None]] = 1
         pSpotNeighb[SpotInCell == 0, -1] = 1
 
         ## Add a couple of checks here
         return pSpotNeighb
 
     def loglik(self, cells, cfg):
-        # area = cells.cell_props['area'][1:]
+        # area = cells.ini_cell_props['area'][1:]
         # mcr = np.mean(np.sqrt(area / np.pi)) * 0.5  # This is the meanCellRadius
         mcr = cells.mcr
         dim = 2  # dimensions of the normal distribution: Bivariate
         # Assume a bivariate normal and calc the likelihood
         D = -self.Dist ** 2 / (2 * mcr ** 2) - dim/2 * np.log(2 * np.pi * mcr ** 2)
 
         # last column (nN-closest) keeps the misreads,
@@ -292,28 +318,44 @@
             ne.evaluate("log(beta)", out=logb)
             return scipy.special.psi(r) - logb
         else:
             return scipy.special.psi(r) - np.log(beta).astype(dtype)
 
 
 # ----------------------------------------Class: SingleCell--------------------------------------------------- #
+# ----------------------------------------Class: SingleCell--------------------------------------------------- #
 class SingleCell(object):
     def __init__(self, scdata: pd.DataFrame, genes: np.array, config):
-        self.raw_data = self._raw_data(scdata, genes)
+        self.isMissing = None  # Will be set to False is single cell data are assumed known and given as an input
+                               # otherwise, if they are unknown, this will be set to True and the algorithm will
+                               # try to estimate them
+        # self.raw_data = self._raw_data(scdata, genes)
         self.config = config
         self._mean_expression, self._log_mean_expression = self._setup(scdata, genes, self.config)
 
     def _setup(self, scdata, genes, config):
         """
         calcs the mean (and the log-mean) gene counts per cell type. Note that
         some hyperparameter values have been applied before those means are derived.
         These hyperparameters and some bacic cleaning takes part in the functions
         called herein
         """
-        me, lme = self._helper(self.raw_data.copy())
+        if scdata is None:
+            logger.info('Single Cell data are missing. Cannot determine meam expression per cell class.')
+            logger.info('We will try to estimate the array instead')
+            logger.info('Starting point is a diagonal array of size numGenes-by-numGenes')
+            # expr = self._naive(scdata, genes)
+            expr = self._diag(genes)
+            self.isMissing = True
+        else:
+            expr = self._raw_data(scdata, genes)
+            self.isMissing = False
+
+        self.raw_data = expr
+        me, lme = self._helper(expr.copy())
         dtype = self.config['dtype']
 
         assert me.columns[-1] == 'Zero', "Last column should be the Zero class"
         assert lme.columns[-1] == 'Zero', "Last column should be the Zero class"
         return me.astype(dtype), lme.astype(dtype)
 
     # -------- PROPERTIES -------- #
@@ -358,14 +400,41 @@
 
         # expr = self.config['Inefficiency'] * arr
         me = expr.rename_axis('gene_name').rename_axis("class_name", axis="columns")  # mean expression
         me = me + self.config['SpotReg']  # add the regularization parameter
         lme = np.log(me)  # log mean expression
         return me, lme
 
+    def _gene_expressions(self, fitted, scale):
+        """
+        Finds the expected mean gene counts. The prior *IS NOT* taken
+        into account. We use data evidence only
+        For the zero class only the prior is used *AND NOT* data
+        evidence.
+        """
+
+        # the prior on mean expression follows a Gamma(m * M , m), where M is the starting point (the initial
+        # array) of single cell data
+        # 07-May-2023. Hiding m from the config.py. Should bring it back at a later version
+        # m = self.config['m']
+        m = 1
+
+        a = fitted + m * (self.raw_data + self.config['SpotReg'])
+        b = scale + m
+        me = a / b
+        lme = scipy.special.psi(a) - np.log(b)
+
+        # the expressions for the zero class are a 0.0 plus the regularition param
+        zero_col = np.zeros(me.shape[0]) + self.config['SpotReg']
+        me = me.assign(Zero=zero_col)
+        # For the mean of the log-expressions, again only the prior is used for the Zero class
+        zero_col_2 = scipy.special.psi(m * zero_col) - np.log(m)
+        lme = lme.assign(Zero=zero_col_2)
+        return me, lme
+
     def _keep_labels_unique(self, scdata):
         """
         In the single cell data you might find cases where two or more rows have the same gene label
         In these cases keep the row with the highest total gene count
         """
 
         # 1. get the row total and assign it to a new column
@@ -398,46 +467,79 @@
         dfT = df.T
 
         logger.info(' Single cell data: Grouping gene counts by cell type. Aggregating function is the mean.')
         out = dfT.groupby(dfT.index.values).agg('mean').T
         logger.info(' Grouped single cell data have %d genes and %d cell types' % (out.shape[0], out.shape[1]))
         return out
 
+    def _diag(self, genes):
+        # logger.info('******************************************************')
+        # logger.info('*************** DIAGONAL SINGLE CELL DATA ***************')
+        # logger.info('******************************************************')
+        nG = len(genes)
+        mgc = 15  # the avg gene count per cell. Better expose that so it can be set by the user.
+        arr = mgc * np.eye(nG)
+        labels = ['class_%d' % (i+1) for i, _ in enumerate(genes)]
+        df = pd.DataFrame(arr).set_index(genes)
+        df.columns = labels
+        return df
 
 # ---------------------------------------- Class: CellType --------------------------------------------------- #
 class CellType(object):
     def __init__(self, single_cell):
         assert single_cell.classes[-1] == 'Zero', "Last label should be the Zero class"
         self._names = single_cell.classes
-        self._prior = None
+        self._alpha = None
+        self.single_cell_data_missing = single_cell.isMissing
 
     @property
     def names(self):
         assert self._names[-1] == 'Zero', "Last label should be the Zero class"
         return self._names
 
     @property
     def nK(self):
         return len(self.names)
 
     @property
-    def prior(self):
-        return self._prior
+    def alpha(self):
+        return self._alpha
 
-    @prior.setter
-    def prior(self, val):
-        self._prior = val
+    @alpha.setter
+    def alpha(self, val):
+        self._alpha = val
 
     @property
-    def log_prior(self):
-        return np.log(self.prior)
+    def pi_bar(self):
+        return self.alpha / self.alpha.sum()
+
+    @property
+    def logpi_bar(self):
+        return scipy.special.psi(self.alpha) - scipy.special.psi(self.alpha.sum())
+
+    @property
+    def prior(self):
+        return self.pi_bar
 
-    def ini_prior(self, ini_family):
-        if ini_family == 'uniform':
-            self.prior = np.append([.5 * np.ones(self.nK - 1) / self.nK], 0.5)
+    @property
+    def log_prior(self):
+        if self.single_cell_data_missing:
+            return self.logpi_bar
         else:
-            raise Exception('Method not implemented yet. Please pass "uniform" when you call ini_prior() ')
+            return np.log(self.prior)
+
+    def size(self, cells):
+        """
+        calcs the size of a cell class, ie how many members (ie cells) each cell type has
+        """
+        return cells.classProb.sum(axis=0)
+
+    def ini_prior(self):
+        self.alpha = self.ini_alpha()
+
+    def ini_alpha(self):
+        return np.append(np.ones(self.nK - 1), sum(np.ones(self.nK - 1)))
```

## pciSeq/src/cell_call/main.py

```diff
@@ -20,15 +20,15 @@
         self.nS = self.spots.nS                         # number of spots
         self.nN = self.config['nNeighbors'] + 1         # number of closest nearby cells, candidates for being parent
                                                         # cell of any given spot. The last cell will be used for the
                                                         # misread spots. (ie cell at position nN is the background)
         self.has_converged = False
 
     def initialise(self):
-        self.cellTypes.ini_prior('uniform')
+        self.cellTypes.ini_prior()
         self.cells.classProb = np.tile(self.cellTypes.prior, (self.nC, 1))
         self.genes.init_eta(1, 1 / self.config['Inefficiency'])
         self.spots.parent_cell_id = self.spots.cells_nearby(self.cells)
         self.spots.parent_cell_prob = self.spots.ini_cellProb(self.spots.parent_cell_id, self.config)
         self.spots.gamma_bar = np.ones([self.nC, self.nG, self.nK]).astype(self.config['dtype'])
 
     # -------------------------------------------------------------------- #
@@ -46,20 +46,27 @@
 
             # 2. calc expected gamma
             self.gamma_upd()
 
             # 3. assign cells to cell types
             self.cell_to_cellType()
 
+            if self.single_cell.isMissing:
+                self.dalpha_upd()
+
             # 4. assign spots to cells
             self.spots_to_cell()
 
             # 5. update gene efficiency
             self.eta_upd()
 
+            # 6. Update single cell data
+            if self.single_cell.isMissing:
+                self.mu_upd()
+
             self.has_converged, delta = utils.hasConverged(self.spots, p0, self.config['CellCallTolerance'])
             logger.info(' Iteration %d, mean prob change %f' % (i, delta))
 
             # replace p0 with the latest probabilities
             p0 = self.spots.parent_cell_prob
 
             if self.has_converged:
@@ -110,15 +117,15 @@
     def gamma_upd(self):
         """
          Implements equation (3) of the Qian paper
         """
         cells = self.cells
         cfg = self.config
         dtype = self.config['dtype']
-        beta = np.einsum('c, gk, g -> cgk', cells.cell_props['area_factor'], self.single_cell.mean_expression, self.genes.eta_bar).astype(dtype) + cfg['rSpot']
+        beta = np.einsum('c, gk, g -> cgk', cells.ini_cell_props['area_factor'], self.single_cell.mean_expression, self.genes.eta_bar).astype(dtype) + cfg['rSpot']
         # beta = np.einsum('c, gk -> cgk', cells.cell_props['area_factor'], self.single_cell.mean_expression).astype(dtype) + cfg['rSpot']
         rho = cfg['rSpot'] + cells.geneCount
 
         self.spots.gamma_bar = self.spots.gammaExpectation(rho, beta)
         self.spots.log_gamma_bar = self.spots.logGammaExpectation(rho, beta)
 
     # -------------------------------------------------------------------- #
@@ -131,15 +138,15 @@
         :param config:
         :return:
         """
 
         # gene_gamma = self.genes.eta
         dtype = self.config['dtype']
         # ScaledExp = np.einsum('c, g, gk -> cgk', self.cells.alpha, self.genes.eta, sc.mean_expression.data) + self.config['SpotReg']
-        ScaledExp = np.einsum('c, g, gk -> cgk', self.cells.cell_props['area_factor'], self.genes.eta_bar, self.single_cell.mean_expression).astype(dtype)
+        ScaledExp = np.einsum('c, g, gk -> cgk', self.cells.ini_cell_props['area_factor'], self.genes.eta_bar, self.single_cell.mean_expression).astype(dtype)
         pNegBin = ScaledExp / (self.config['rSpot'] + ScaledExp)
         cgc = self.cells.geneCount
         contr = utils.negBinLoglik(cgc, self.config['rSpot'], pNegBin)
         wCellClass = np.sum(contr, axis=1) + self.cellTypes.log_prior
         pCellClass = utils.softmax(wCellClass, axis=1)
 
         ## self.cells.classProb = pCellClass
@@ -195,15 +202,15 @@
         """
         grand_total = self.cells.background_counts.sum() + self.cells.total_counts.sum()
         assert round(grand_total) == self.spots.data.shape[0], \
             'The sum of the background spots and the total gene counts should be equal to the number of spots'
 
         classProb = self.cells.classProb
         mu = self.single_cell.mean_expression
-        area_factor = self.cells.cell_props['area_factor']
+        area_factor = self.cells.ini_cell_props['area_factor']
         gamma_bar = self.spots.gamma_bar
 
         zero_prob = classProb[:, -1]  # probability a cell being a zero expressing cell
         zero_class_counts = self.spots.zero_class_counts(self.spots.gene_id, zero_prob)
 
         # Calcs the sum in the Gamma distribution (equation 5). The zero class
         # is excluded from the sum, hence the arrays in the einsum below stop at :-1
@@ -219,13 +226,84 @@
         beta = self.config['rGene']/self.config['Inefficiency'] + class_total_counts
         # res = num / denom
 
         # Finally, update gene_gamma
         self.genes.calc_eta(alpha, beta)
         # self.genes.eta_bar = res.values
 
+    # -------------------------------------------------------------------- #
+    def mu_upd(self):
+        # logger.info('Update single cell data')
+        # # make an array nS-by-nN and fill it with the spots id
+        # gene_ids = np.tile(self.spots.gene_id, (self.nN, 1)).T
+        #
+        # # flatten it
+        # gene_ids = gene_ids.ravel()
+        #
+        # # make corresponding arrays for cell_id and probs
+        # cell_ids = self.spots.parent_cell_id.ravel()
+        # probs = self.spots.parent_cell_prob.ravel()
+        #
+        # # make the array to be used as index in the group-by operation
+        # group_idx = np.vstack((cell_ids, gene_ids))
+        #
+        # # For each cell aggregate the number of spots from the same gene.
+        # # It will produce an array of size nC-by-nG where the entry at (c,g)
+        # # is the gene counts of gene g within cell c
+        # N_cg = npg.aggregate(group_idx, probs, size=(self.nC, self.nG))
+
+        classProb = self.cells.classProb[1:, :-1].copy()
+        geneCount = self.cells.geneCount[1:, :].copy()
+        gamma_bar = self.spots.gamma_bar[1:, :, :-1].copy()
+        area_factor = self.cells.ini_cell_props['area_factor'][1:]
+
+        numer = np.einsum('ck, cg -> gk', classProb, geneCount)
+        denom = np.einsum('ck, c, cgk, g -> gk', classProb, area_factor, gamma_bar, self.genes.eta_bar)
+
+        # set the hyperparameter for the gamma prior
+        # m = 1
+
+        # ignore the last class, it is the zero class
+        # numer = numer[:, :-1]
+        # denom = denom[:, :-1]
+        # mu_gk = (numer + m * self.single_cell.raw_data) / (denom + m)
+        me, lme = self.single_cell._gene_expressions(numer, denom)
+        self.single_cell._mean_expression = me
+        self.single_cell._log_mean_expression = lme
+
+        # logger.info('Single cell data updated')
+
+    # -------------------------------------------------------------------- #
+    def dalpha_upd(self):
+        # logger.info('Update cell type (marginal) distribution')
+        zeta = self.cells.classProb.sum(axis=0)  # this the class size
+        alpha = self.cellTypes.ini_alpha()
+        out = zeta + alpha
+
+        # logger.info("***************************************************")
+        # logger.info("**** Dirichlet alpha is zero if class size <=%d ****" % self.config['min_class_size'])
+        # logger.info("***************************************************")
+
+        # 07-May-2013: Hiding 'min_class_size' from the config file. Should bring it back at a later version
+        # mask = zeta <= self.config['min_class_size']
+        min_class_size = 5
+        mask = zeta <= min_class_size
+
+        # make sure Zero class is the last one
+        assert self.cellTypes.names[-1] == "Zero"
+        assert len(self.cellTypes.names) == len(mask)
+
+        # make sure the last value ie always False, overriding if necessary the
+        # check a few lines when the mask variable was set.
+        # In this manner we will prevent the Zero class from being removed.
+        mask[-1] = False
+
+        # If a class size is smaller than 'min_class_size' then it will be assigned a weight of almost zero
+        out[mask] = 10e-6
+        self.cellTypes.alpha = out
+
```

## pciSeq/src/cell_call/summary.py

```diff
@@ -5,17 +5,17 @@
 
 def _iss_summary(cells, genes, single_cell):
     '''
     returns a dataframe summarising the main features of each cell, ie gene counts and cell types
     :param spots:
     :return:
     '''
-    x = cells.cell_props['x']
-    y = cells.cell_props['y']
-    cell_id = cells.cell_props['cell_label']
+    # x = cells.cell_props['x']
+    # y = cells.cell_props['y']
+    cell_id = cells.ini_cell_props['cell_label']
 
     gene_count = cells.geneCount
     class_prob = cells.classProb
     gene_names = genes.gene_panel
     class_names = single_cell.classes
 
     tol = 0.001
@@ -26,17 +26,17 @@
     name_list = [gene_names[isCount_nonZero[n]].tolist() for n in range(N)]
     count_list = [gene_count[n, isCount_nonZero[n]].tolist() for n in range(N)]
 
     isProb_nonZero = [class_prob[n, :] > tol for n in range(N)]
     class_name_list = [class_names[isProb_nonZero[n]].tolist() for n in range(N)]
     prob_list = [class_prob[n, isProb_nonZero[n]].tolist() for n in range(N)]
 
-    iss_df = pd.DataFrame({'Cell_Num': cells.cell_props['cell_label'].tolist(),
-                           'X': cells.cell_props['x'].tolist(),
-                           'Y': cells.cell_props['y'].tolist(),
+    iss_df = pd.DataFrame({'Cell_Num': cells.centroid.index.tolist(),
+                           'X': cells.centroid['x'].tolist(),
+                           'Y': cells.centroid['y'].tolist(),
                            'Genenames': name_list,
                            'CellGeneCount': count_list,
                            'ClassName': class_name_list,
                            'Prob': prob_list
                             },
                            columns=['Cell_Num', 'X', 'Y', 'Genenames', 'CellGeneCount', 'ClassName', 'Prob']
                            )
```

## pciSeq/src/cell_call/utils.py

```diff
@@ -263,13 +263,13 @@
         sys.stderr.write('Downloading: "{}" to {}\n'.format(url, filename))
         download_url_to_file(url, filename)
     return filename
 
 
 def get_out_dir(path, sub_folder=''):
     if path[0] == 'default':
-        out_dir = os.path.join(tempfile.gettempdir(), 'pciSeq', sub_folder)
+        out_dir = os.path.join(tempfile.gettempdir(), 'pciSeq')
     else:
-        out_dir = os.path.join(path[0], sub_folder)
+        out_dir = os.path.join(path[0], sub_folder, 'pciSeq')
     if not os.path.exists(out_dir):
         os.makedirs(out_dir)
     return out_dir
```

## pciSeq/src/preprocess/cell_borders.py

```diff
@@ -1,211 +1,35 @@
-""" Functions for extracting the boundaries of the cells """
-
-# WARNING -- WARNING -- WARNING
-# NOTE: 30-Nov-2020: I am using pydip to get the cell boundaries. It is a lot faster but I need to
-# further test this and compare the cell boundaries with the previous way i used to do it
-
-import cv2
+""" Functions to extract the cell boundaries """
 import pandas as pd
 import numpy as np
-from scipy.sparse import coo_matrix
-from scipy.ndimage import binary_erosion
-from multiprocessing.dummy import Pool as ThreadPool
-from multiprocessing import cpu_count
 import diplib as dip
-import time
-import logging
-
-# logging.basicConfig(
-#     level=logging.INFO,
-#     format="%(asctime)s:%(levelname)s:%(message)s"
-# )
-#
-# logger = logging.getLogger()
-
-
-
-def cell_boundaries(stage, cell_props):
-    '''
-    calculate the outlines of the cells
-    :return:
-    '''
-
-    # loop over the self.cell_props
-    res_list = []
-    for tile in stage.tiles:
-        if np.any(tile['label_image'].data):
-            df = obj_outline(tile, cell_props)
-            res_list.append(df)
-        else:
-            logger.info('tile:%d empty, No cells to draw boundaries were found' % tile['tile_id'])
-    _df = pd.concat(res_list).astype({"label": int})
-
-    # make a Dataframe to keep boundaries of the cells which are not clipped by the tile
-    df_1 = _df.iloc[np.isin(_df.label, cell_props[~cell_props.is_clipped].label)]
-
-    # get the labels of the clipped cells
-    in_multiple_tiles = sorted(cell_props[cell_props.is_clipped].label.values)
-    logger.info('There are %d cells whose boundaries span across multiple tiles' % len(in_multiple_tiles))
-
-    # find the boundaries of the clipped cells
-    _list = collate_borders_par(stage, in_multiple_tiles)
-    df_2 = pd.DataFrame(_list.items(), columns=['label', 'coords'])\
-        .astype({"label": int})
-    # df_2 = pd.DataFrame(_list).astype({"label": int})
-
-    # Both clipped and unclipped in a dataframe
-    res = pd.concat([df_1, df_2])
-
-    set_diff = set(cell_props.label) - set(res.label.values)
-    if set_diff:
-        unresolved_labels = pd.DataFrame({'label': list(set_diff), 'coords': np.nan * np.ones(len(set_diff))})
-        res = pd.concat([res, unresolved_labels])
-
-    # assert set(_df.label.values) == set(res.label.values)
-    assert res.shape[0] == cell_props.shape[0]
-    assert np.all(sorted(res.label) == sorted(cell_props.label))
-    assert np.unique(res.label).size == res.shape[0], 'Array cannot have duplicates'
-    return res.sort_values(['label'], ascending=[True])
-
-
-
-def collate_borders_par(stage, labels):
-    n = max(1, cpu_count() - 1)
-    pool = ThreadPool(16)
-    results = {}
-    pool.map(collate_borders_helper(stage, results), labels)
-    pool.close()
-    pool.join()
-    return results
-
-def collate_borders_helper(stage, results):
-    def inner_fun(label):
-        logger.info('label: %d. Finding the cell boundaries' % label)
-        label_image = stage.collate_arrays(stage.merge_register[label])
-        offset_x, offset_y = stage.find_offset(stage.merge_register[label])
-        results[label] = np.array(get_label_contours(label_image, label, offset_x, offset_y))
-    return inner_fun
-
-def obj_outline(tile, cell_props):
-    logger.info('Getting cell boundaries for cells in tile: %d' % tile['tile_id'])
-    label_image = tile['label_image'].toarray()
-    offset_x = tile['tile_offset_x']
-    offset_y = tile['tile_offset_y']
-    clipped_cells = cell_props[cell_props.is_clipped].label.values
-
-    df = extract_borders_dip(label_image.astype(np.uint64), offset_x, offset_y, clipped_cells)
-    return df
-
-
-def extract_borders(label_image, offset_x, offset_y, clipped_labels):
-    """
-    same as ''extract_borders_par'' but without parallelism
-    :param label_image:
-    :return:
-    """
-    labels = sorted(set(label_image.flatten()) - {0} - set(clipped_labels))
-    out = {}
-    for label in labels:
-        y = label_image == label
-        y = y * 255
-        y = y.astype('uint8')
-        contours, hierarchy = cv2.findContours(y, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE, offset=(offset_x, offset_y))
-        contours = np.squeeze(contours)
-        out[label] = contours.tolist()
-    out = pd.DataFrame([out]).T
-    out = out.reset_index()
-    out.columns = ['label', 'coords']
-    return out
-
 
-def extract_borders_par(label_image, offset_x, offset_y, clipped_labels):
-    """ Extracts the borders of the objects from the label_image
 
-    :param label_image: a 2-d array, the same size as the image, where the value at position (i,j) denotes the label of
-                        the object that corresponds at pixel position (i, j) of the image. If a pixel is on the background
-                        then the label is zero
-    :param offset_x: The x-coordinate of the top left corner of the image
-    :param offset_y: The y-coordinate of the top left corner of the image
-    :param clipped_labels: a list containing the labels of the cells that will be excluded from border extraction
-    :return: A dataframe with columns '''label''' and '''coords'''. Column '''label''' contains the label of the cell
-            and column '''coords''' contains a list of pairs describing the coordinates of the cell boundaries/contours.
-            Each such pair is a list.
+def extract_borders_dip(label_image, offset_x=0, offset_y=0, exclude_labels=(0,)):
     """
-
-    labels = sorted(set(label_image.flatten()) - {0} - set(clipped_labels))
-    out = extract_borders_helper([label_image, offset_x, offset_y], labels)
-    out = pd.DataFrame([out]).T
-    out = out.reset_index()
-    out.columns = ['label', 'coords']
-    return out.sort_values(by=['label'])
-
-
-def extract_borders_helper(args, labels):
-    n = max(1, cpu_count() - 1)
-    pool = ThreadPool(n)
-    results = {}
-    pool.map(wrapper_helper(args, results), labels)
-    pool.close()
-    pool.join()
-    return results
-
-
-def wrapper_helper(argsin, results):
-    '''
-    closure function to pass parameters to the function called by pool.map
-    :param argsin:
-    :param results:
-    :return:
-    '''
-    def inner_fun(label):
-        label_image = argsin[0]
-        offset_x = argsin[1]
-        offset_y = argsin[2]
-        contours = get_label_contours(label_image, label, offset_x, offset_y)
-        results[label] = contours
-    return inner_fun
-
-
-def get_label_contours(label_image, label, offset_x, offset_y):
-    '''
-    reads a label_image and gets the boundaries of the cell labeled by ''label''
-    :param label_image:
-    :param label:
-    :param offset_x:
-    :param offset_y:
-    :return:
-    '''
-    img = label_image == label
-    img = img * 255
-    img = img.astype('uint8')
-    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE, offset=(offset_x, offset_y))
-    contours = np.squeeze(contours)
-    return contours.tolist()
-
-
-def extract_borders_dip(label_image, offset_x, offset_y, clipped_labels):
-    """
-    NOTES: # using Simplify drastically reduces the array that describes the polygon boundaries but you
-             might end up with a slightly different polygon. The difference is only on a very few pixels.
-             I do not know which one reflect the actual boundaries more closely, but using Simplify and
-             reduce the size of the boudaries array is convenient. Also having few different pixels is not
-             crucial, hence I think keeping Simplify makes sense
+    Extracts the cell boundaries from the label image array. The background is
+    assumed to have label=0 and it will be ignored by default.
     Parameters
     ----------
-    label_image
-    offset_x
-    offset_y
-    clipped_labels
+    label_image:    The label image array, typically obtained from some image segmentation
+                    application and maps every pixel on the image to a cell label.
+    offset_x:       Amount to shift the boundaries along the x-axis
+    offset_y:       Amount to shift the boundaries along the y-axis
+    exclude_labels: Array-like, contains the labels to be ignored.
 
     Returns
     -------
-
+    Returns a dataframe with columns ['labels', 'coords'] where column 'coords' keeps a
+    list like [[x0, y0], [x1, y2],...,[x0, y0]] of the (closed-loop) boundaries coordinates
+    for corresponding cell  label
     """
-    labels = sorted(set(label_image.flatten()) - {0} - set(clipped_labels))
+
+    if exclude_labels is None:
+        exclude_labels = [0]
+    labels = sorted(set(label_image.flatten()) - set(exclude_labels))
     cc = dip.GetImageChainCodes(label_image)  # input must be an unsigned integer type
     d = {}
     for c in cc:
         if c.objectID in labels:
             # p = np.array(c.Polygon())
             p = c.Polygon().Simplify()
             p = p + np.array([offset_x, offset_y])
@@ -216,40 +40,206 @@
             pass
     df = pd.DataFrame([d]).T
     df = df.reset_index()
     df.columns = ['label', 'coords']
     return df
 
 
-def outline_fix(label_image):
-    res_list = []
-    coo = coo_matrix(label_image)
-    labels = np.unique(coo.data)
-    for label in sorted(set(labels)):
-        # print('label: %d' % label)
-        c = coo.copy()
-        c.data[c.data != label] = 0
-        c = c.toarray()
-        mask = binary_erosion(c)
-        c[mask] = 0
-        c = coo_matrix(c)
-        if c.data.size > 0:
-            df = pd.DataFrame({'coords': list(zip(c.col, c.row)), 'label': c.data})
-            df = df.groupby('label')['coords'].apply(lambda group_series: group_series.tolist()).reset_index()
-            df = df.astype({"label": int})
-        else:
-            df = pd.DataFrame()
-        res_list.append(df)
-
-    if res_list:
-        out = pd.concat(res_list).astype({"label": int})
-    else:
-        out = pd.DataFrame()
-
-    return out
+# *********************************************************************************
+# All Functions below are all deprecated. WIll be cleaned-p at a later verion
+# *********************************************************************************
+#
+# def cell_boundaries(stage, cell_props):
+#     '''
+#     calculate the outlines of the cells
+#     :return:
+#     '''
+#
+#     # loop over the self.cell_props
+#     res_list = []
+#     for tile in stage.tiles:
+#         if np.any(tile['label_image'].data):
+#             df = obj_outline(tile, cell_props)
+#             res_list.append(df)
+#         else:
+#             logger.info('tile:%d empty, No cells to draw boundaries were found' % tile['tile_id'])
+#     _df = pd.concat(res_list).astype({"label": int})
+#
+#     # make a Dataframe to keep boundaries of the cells which are not clipped by the tile
+#     df_1 = _df.iloc[np.isin(_df.label, cell_props[~cell_props.is_clipped].label)]
+#
+#     # get the labels of the clipped cells
+#     in_multiple_tiles = sorted(cell_props[cell_props.is_clipped].label.values)
+#     logger.info('There are %d cells whose boundaries span across multiple tiles' % len(in_multiple_tiles))
+#
+#     # find the boundaries of the clipped cells
+#     _list = collate_borders_par(stage, in_multiple_tiles)
+#     df_2 = pd.DataFrame(_list.items(), columns=['label', 'coords'])\
+#         .astype({"label": int})
+#     # df_2 = pd.DataFrame(_list).astype({"label": int})
+#
+#     # Both clipped and unclipped in a dataframe
+#     res = pd.concat([df_1, df_2])
+#
+#     set_diff = set(cell_props.label) - set(res.label.values)
+#     if set_diff:
+#         unresolved_labels = pd.DataFrame({'label': list(set_diff), 'coords': np.nan * np.ones(len(set_diff))})
+#         res = pd.concat([res, unresolved_labels])
+#
+#     # assert set(_df.label.values) == set(res.label.values)
+#     assert res.shape[0] == cell_props.shape[0]
+#     assert np.all(sorted(res.label) == sorted(cell_props.label))
+#     assert np.unique(res.label).size == res.shape[0], 'Array cannot have duplicates'
+#     return res.sort_values(['label'], ascending=[True])
+#
+#
+#
+# def collate_borders_par(stage, labels):
+#     n = max(1, cpu_count() - 1)
+#     pool = ThreadPool(16)
+#     results = {}
+#     pool.map(collate_borders_helper(stage, results), labels)
+#     pool.close()
+#     pool.join()
+#     return results
+#
+# def collate_borders_helper(stage, results):
+#     def inner_fun(label):
+#         logger.info('label: %d. Finding the cell boundaries' % label)
+#         label_image = stage.collate_arrays(stage.merge_register[label])
+#         offset_x, offset_y = stage.find_offset(stage.merge_register[label])
+#         results[label] = np.array(get_label_contours(label_image, label, offset_x, offset_y))
+#     return inner_fun
+#
+# def obj_outline(tile, cell_props):
+#     logger.info('Getting cell boundaries for cells in tile: %d' % tile['tile_id'])
+#     label_image = tile['label_image'].toarray()
+#     offset_x = tile['tile_offset_x']
+#     offset_y = tile['tile_offset_y']
+#     clipped_cells = cell_props[cell_props.is_clipped].label.values
+#
+#     df = extract_borders_dip(label_image.astype(np.uint64), offset_x, offset_y, clipped_cells)
+#     return df
+#
+#
+# def extract_borders(label_image, offset_x, offset_y, clipped_labels):
+#     """
+#     same as ''extract_borders_par'' but without parallelism
+#     :param label_image:
+#     :return:
+#     """
+#     labels = sorted(set(label_image.flatten()) - {0} - set(clipped_labels))
+#     out = {}
+#     for label in labels:
+#         y = label_image == label
+#         y = y * 255
+#         y = y.astype('uint8')
+#         contours, hierarchy = cv2.findContours(y, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE, offset=(offset_x, offset_y))
+#         contours = np.squeeze(contours)
+#         out[label] = contours.tolist()
+#     out = pd.DataFrame([out]).T
+#     out = out.reset_index()
+#     out.columns = ['label', 'coords']
+#     return out
+#
+#
+# def extract_borders_par(label_image, offset_x, offset_y, clipped_labels):
+#     """ Extracts the borders of the objects from the label_image
+#
+#     :param label_image: a 2-d array, the same size as the image, where the value at position (i,j) denotes the label of
+#                         the object that corresponds at pixel position (i, j) of the image. If a pixel is on the background
+#                         then the label is zero
+#     :param offset_x: The x-coordinate of the top left corner of the image
+#     :param offset_y: The y-coordinate of the top left corner of the image
+#     :param clipped_labels: a list containing the labels of the cells that will be excluded from border extraction
+#     :return: A dataframe with columns '''label''' and '''coords'''. Column '''label''' contains the label of the cell
+#             and column '''coords''' contains a list of pairs describing the coordinates of the cell boundaries/contours.
+#             Each such pair is a list.
+#     """
+#
+#     labels = sorted(set(label_image.flatten()) - {0} - set(clipped_labels))
+#     out = extract_borders_helper([label_image, offset_x, offset_y], labels)
+#     out = pd.DataFrame([out]).T
+#     out = out.reset_index()
+#     out.columns = ['label', 'coords']
+#     return out.sort_values(by=['label'])
+#
+#
+# def extract_borders_helper(args, labels):
+#     n = max(1, cpu_count() - 1)
+#     pool = ThreadPool(n)
+#     results = {}
+#     pool.map(wrapper_helper(args, results), labels)
+#     pool.close()
+#     pool.join()
+#     return results
+#
+#
+# def wrapper_helper(argsin, results):
+#     '''
+#     closure function to pass parameters to the function called by pool.map
+#     :param argsin:
+#     :param results:
+#     :return:
+#     '''
+#     def inner_fun(label):
+#         label_image = argsin[0]
+#         offset_x = argsin[1]
+#         offset_y = argsin[2]
+#         contours = get_label_contours(label_image, label, offset_x, offset_y)
+#         results[label] = contours
+#     return inner_fun
+#
+#
+# def get_label_contours(label_image, label, offset_x, offset_y):
+#     '''
+#     reads a label_image and gets the boundaries of the cell labeled by ''label''
+#     :param label_image:
+#     :param label:
+#     :param offset_x:
+#     :param offset_y:
+#     :return:
+#     '''
+#     img = label_image == label
+#     img = img * 255
+#     img = img.astype('uint8')
+#     contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE, offset=(offset_x, offset_y))
+#     contours = np.squeeze(contours)
+#     return contours.tolist()
+#
+#
+#
+#
+#
+# def outline_fix(label_image):
+#     res_list = []
+#     coo = coo_matrix(label_image)
+#     labels = np.unique(coo.data)
+#     for label in sorted(set(labels)):
+#         # print('label: %d' % label)
+#         c = coo.copy()
+#         c.data[c.data != label] = 0
+#         c = c.toarray()
+#         mask = binary_erosion(c)
+#         c[mask] = 0
+#         c = coo_matrix(c)
+#         if c.data.size > 0:
+#             df = pd.DataFrame({'coords': list(zip(c.col, c.row)), 'label': c.data})
+#             df = df.groupby('label')['coords'].apply(lambda group_series: group_series.tolist()).reset_index()
+#             df = df.astype({"label": int})
+#         else:
+#             df = pd.DataFrame()
+#         res_list.append(df)
+#
+#     if res_list:
+#         out = pd.concat(res_list).astype({"label": int})
+#     else:
+#         out = pd.DataFrame()
+#
+#     return out
 
 
 
 if __name__ == "__main__":
     dummy_img = np.array([
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
@@ -265,34 +255,34 @@
         [0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 0, 0, 0],
         [0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 0, 0, 0],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
     ])
 
-    fov51 = pd.read_csv('../../fov51.csv', header=None)
-    fov51 = fov51.values
-
-    fov56 = pd.read_csv('../../fov56.csv', header=None)
-    fov56 = fov56.values
-
-    # x = dummy_img
-    x = fov56
-
-    start = time.time()
-    out = extract_borders(x, 0, 0, [0])
-    print(time.time() - start)
-
+    # fov51 = pd.read_csv('../../fov51.csv', header=None)
+    # fov51 = fov51.values
+    #
+    # fov56 = pd.read_csv('../../fov56.csv', header=None)
+    # fov56 = fov56.values
+    #
+    # # x = dummy_img
+    # x = fov56
+    #
     # start = time.time()
-    # out2 = outline_fix(x)
+    # out = extract_borders(x, 0, 0, [0])
     # print(time.time() - start)
-
-    start = time.time()
-    # out3 = extract_borders(x, 0, 0, [0])
-    out3 = extract_borders_par(x, 0, 0, [0])
-    print(time.time() - start)
-
-    print(out)
-    # print(out2)
-    print(out3)
-
-    print('Done')
+    #
+    # # start = time.time()
+    # # out2 = outline_fix(x)
+    # # print(time.time() - start)
+    #
+    # start = time.time()
+    # # out3 = extract_borders(x, 0, 0, [0])
+    # out3 = extract_borders_par(x, 0, 0, [0])
+    # print(time.time() - start)
+    #
+    # print(out)
+    # # print(out2)
+    # print(out3)
+    #
+    # print('Done')
```

## pciSeq/src/preprocess/spot_labels.py

```diff
@@ -74,25 +74,25 @@
 
     # 2. Get cell centroids and area
     props = skmeas.regionprops(coo.toarray().astype(np.int32))
     props_df = pd.DataFrame(data=[(d.label, d.area, d.centroid[1], d.centroid[0]) for d in props],
                       columns=['label', 'area', 'x_cell', 'y_cell'])
 
     # 3. Get the cell boundaries
-    cell_boundaries = extract_borders_dip(coo.toarray().astype(np.uint32), 0, 0, [0])
+    cell_boundaries = extract_borders_dip(coo.toarray().astype(np.uint32))
 
     assert props_df.shape[0] == cell_boundaries.shape[0] == np.unique(coo.data).shape[0]
     assert set(spots.label[spots.label > 0]) <= set(props_df.label)
 
     cells = props_df.merge(cell_boundaries)
     cells.sort_values(by=['label', 'x_cell', 'y_cell'])
     assert cells.shape[0] == cell_boundaries.shape[0] == props_df.shape[0]
 
     # join spots and cells on the cell label so you can get the x,y coords of the cell for any given spot
     spots = spots.merge(cells, how='left', on=['label'])
 
-    _cells = cells[['label', 'area', 'x_cell', 'y_cell']].rename(columns={'x_cell': 'x', 'y_cell': 'y'})
+    _cells = cells[['label', 'area', 'x_cell', 'y_cell']].rename(columns={'x_cell': 'x0', 'y_cell': 'y0'})
     _cell_boundaries = cells[['label', 'coords']].rename(columns={'label': 'cell_id'})
     _spots = spots[['x', 'y', 'label', 'Gene', 'x_cell', 'y_cell']].rename(columns={'Gene': 'target', 'x': 'x_global', 'y': 'y_global'})
 
     return _cells, _cell_boundaries, _spots
```

## pciSeq/src/viewer/run_flask.py

```diff
@@ -56,17 +56,9 @@
     def index():
         return render_template("index.html", data=None)
 
     Timer(1, get_browser, [port]).start()
     flask_app.run(port=port, debug=False)
 
 
-def mk_ini(cellData, geneData):
-    ini = {
-        'cellData': cellData.to_json(orient='records'),
-        'geneData': geneData.to_json(orient='records')
-    }
-    return ini
-
-
 if __name__ == "__main__":
     flask_app_start()
```

## pciSeq/src/viewer/utils.py

```diff
@@ -1,22 +1,89 @@
-""" Functions to manipulate flat files (split or minify them) """
-from typing import Union
 import pandas as pd
 import numpy as np
+import subprocess
+from email.parser import BytesHeaderParser
+import shutil
 import json
 import os
 import glob
 import csv
-import logging
+from pciSeq.src.cell_call.log_config import logger
 
-logger = logging.getLogger(__name__)
 
-from pciSeq import check_libvips
-if check_libvips():
-    import pyvips
+def make_config_base(dst):
+    cellData_tsv = os.path.join(dst, 'data', 'cellData.tsv')
+    geneData_tsv = os.path.join(dst, 'data', 'geneData.tsv')
+
+    cellData_dict = {"mediaLink": "../../data/cellData.tsv", "size": str(os.path.getsize(cellData_tsv))}
+    geneData_dict = {"mediaLink": "../../data/geneData.tsv", "size": str(os.path.getsize(geneData_tsv))}
+
+    return {
+        'cellData': cellData_dict,
+        'geneData': geneData_dict,
+    }
+
+
+def make_config_js(dst, w, h):
+    appDict = make_config_base(dst)
+    cellBoundaries_tsv = os.path.join(dst, 'data', 'cellBoundaries.tsv')
+    cellBoundaries_dict = {"mediaLink": "../../data/cellBoundaries.tsv",
+                           "size": str(os.path.getsize(cellBoundaries_tsv))}
+    roi_dict = {"x0": 0, "x1": w, "y0": 0, "y1": h}
+    appDict['cellBoundaries'] = cellBoundaries_dict
+    appDict['roi'] = roi_dict
+    appDict['zoomLevels'] = 10
+    appDict['tiles'] = "https://storage.googleapis.com/ca1-data/img/262144px/{z}/{y}/{x}.jpgZZZ"
+
+    config_str = "// NOTES: \n" \
+                 "// 1. paths are with respect to the location of 'streaming-tsv-parser.js \n" \
+                 "// 2. roi is the image size in pixels. Leave x0 and y0 at zero and set x1 to the width and y1 to the height \n" \
+                 "// 3. tiles should point to the folder that keeps your pyramid of tiles. If you do not have that just \n" \
+                 "//    change the link to a blind one (change the jpg extension for example). The viewer should work \n" \
+                 "//    without the dapi background though \n" \
+                 "// 4. size is the tsv size in bytes. I use os.path.getsize() to get it. Not crucial if you \n" \
+                 "//    dont get it right, ie the full tsv will still be parsed despite this being wrong. It \n" \
+                 "//    is used by the loading page piecharts to calc how far we are \n" \
+                 "// 5. Leave zoomLevels to 10 \n" \
+                 " function config() { return %s }" % json.dumps(appDict)
+    config = os.path.join(dst, 'viewer', 'js', 'config.js')
+    with open(config, 'w') as data:
+        data.write(str(config_str))
+    logger.info(' viewer config saved at %s' % config)
+
+
+def make_classConfig_js(labels, dst):
+    colours = ["#f3c300", "#875692", "#f38400", "#a1caf1", "#be0032",
+               "#c2b280", "#848482", "#008856", "#e68fac", "#0067a5",
+               "#f99379", "#604e97", "#f6a600", "#b3446c", "#dcd300",
+               "#882d17", "#8db600", "#654522", "#e25822", "#2b3d26"]
+    n = len(colours)
+    config_dict = [{'className': labels[i],
+                   'IdentifiedType': labels[i],
+                   'color': colours[i % n]}
+                  for i, v in enumerate(labels)]
+    config_dict.append({'className': 'Zero', 'IdentifiedType': 'Zero', 'color': '#000000'})
+    config_dict.append({'className': 'Other', 'IdentifiedType': 'Other', 'color': '#C0C0C0'})
+    config_str = " function classColorsCodes() { return %s }" % json.dumps(config_dict)
+    config = os.path.join(dst, 'viewer', 'js', 'classConfig.js')
+    with open(config, 'w') as data:
+        data.write(str(config_str))
+    logger.info(' classConfig saved at %s' % config)
+
+
+def copy_viewer_code(cfg, dst):
+    p = subprocess.run(['pip', 'show', 'pciSeq'], stdout=subprocess.PIPE)
+    h = BytesHeaderParser().parsebytes(p.stdout)
+    pciSeq_dir = os.path.join(h['Location'], 'pciSeq')
+    dim = '2D'
+    src = os.path.join(pciSeq_dir, 'static', dim)
+
+    shutil.copytree(src, dst, dirs_exist_ok=True)
+    logger.info(' viewer code (%s) copied from %s to %s' % (dim, src, dst))
+    return dst
 
 
 def splitter_mb(df, dir_path, mb_size):
     """ Splits a text file in (almost) equally sized parts on the disk. Assumes that there is a header in the first line
     :param filepath: The path of the text file to be broken up into smaller files
     :param mb_size: size in MB of each chunk
     :return:
@@ -34,16 +101,16 @@
     header_line = df.columns.tolist()
     # header_line = next(handle)[1].tolist()
     file_out, handle_out = _get_file(dir_path, n, header_line)
     # data_row = next(handle)[1].tolist()
     for index, row in df.iterrows():
         row = row.tolist()
         size = os.stat(file_out).st_size
-        if size > mb_size*1024*1024:
-            logger.info('saved %s with file size %4.3f MB' % (file_out, size/(1024*1024)))
+        if size > mb_size * 1024 * 1024:
+            logger.info('saved %s with file size %4.3f MB' % (file_out, size / (1024 * 1024)))
             n += 1
             handle_out.close()
             file_out, handle_out = _get_file(dir_path, n, header_line)
         write = csv.writer(handle_out, delimiter='\t')
         write.writerow(row)
 
     # print(str(file_out) + " file size = \t" + str(size))
@@ -68,16 +135,16 @@
             os.remove(f)
 
     n = 0
     header_line = next(handle)
     file_out, handle_out = _get_file(OUT_DIR, filepath, n, header_line)
     for line in handle:
         size = os.stat(file_out).st_size
-        if size > mb_size*1024*1024:
-            print('saved %s with file size %4.3f MB' % (file_out, size/(1024*1024)))
+        if size > mb_size * 1024 * 1024:
+            print('saved %s with file size %4.3f MB' % (file_out, size / (1024 * 1024)))
             n += 1
             handle_out.close()
             file_out, handle_out = _get_file(OUT_DIR, filepath, n, header_line)
         handle_out.write(str(line))
 
     # print(str(file_out) + " file size = \t" + str(size))
     print('saved %s with file size %4.3f MB' % (file_out, size / (1024 * 1024)))
@@ -102,161 +169,17 @@
     else:
         df = None
 
     df_list = np.array_split(df, n)
     if not os.path.exists(OUT_DIR):
         os.makedirs(OUT_DIR)
     else:
-        files = glob.glob(OUT_DIR + '/*.'+ext)
+        files = glob.glob(OUT_DIR + '/*.' + ext)
         for f in files:
             os.remove(f)
 
     for i, d in enumerate(df_list):
         fname = os.path.join(OUT_DIR, filename + '_%d.%s' % (i, ext))
         if ext == 'json':
-            d.to_json(fname,  orient='records')
+            d.to_json(fname, orient='records')
         elif ext == 'tsv':
             d.to_csv(fname, sep='\t', index=False)
-
-
-def crush_data(d):
-    """ Minifies the flatfiles that will be fed to the viewer.
-    I keep only 3 decimal points and no more than the top 10 cell classes as
-    possible assignments for any given cell
-    """
-
-    cell_min = None
-    gene_min = None
-    n = 10 # for each cell keep only the top10 possible cell types. Discard the rest. USE THIS WITH CAUTION. MAYBE IT IS NOT A GOOD IDEA AND I SHOULD KEEP ALL POSSIBLE CELL TYPES
-    try:
-        cellData_path = d['cellData']
-        cell_min = _crush_cellData(cellData_path, n)
-    except KeyError as e:
-        print('key doesnt exist...')
-
-    try:
-        geneData_path = d['geneData']
-        gene_min = _crush_geneData(geneData_path)
-    except KeyError as e:
-        print('key doesnt exist...')
-
-    return [cell_min, gene_min]
-
-
-def _crush_cellData(filepath, n):
-    filename_ext = os.path.basename(filepath)
-    [filename, ext] = filename_ext.split('.')
-
-    cellData = pd.read_csv(filepath, sep='\t')
-    temp = _order_prob(cellData, n)
-    cellData['ClassName'] = temp[0]
-    cellData['Prob'] = temp[1]
-
-    cellData['Prob'] = _round_data2(cellData, 'Prob')
-    cellData['CellGeneCount'] = _round_data(cellData, 'CellGeneCount')
-
-    out_dir: Union[bytes, str] = os.path.join(os.path.dirname(filepath), filename + '_min')
-    target_path = os.path.join(out_dir, filename_ext)
-    _clean_dir(out_dir, ext)
-
-    cellData.to_csv(target_path, sep='\t', index=False)
-    print('Minimised file saved at: %s' % target_path)
-
-    return target_path
-
-
-def _crush_geneData(filepath):
-    filename_ext = os.path.basename(filepath)
-    [filename, ext] = filename_ext.split('.')
-
-    geneData = pd.read_csv(filepath, sep='\t')
-
-    # 1. First, do geneData
-    # use int for the coordinates, not floats
-    geneData = geneData.astype({'x': 'int32', 'y': 'int32'})
-    geneData['neighbour_prob'] = _round_data(geneData, 'neighbour_prob')
-
-    out_dir = os.path.join(os.path.dirname(filepath), filename + '_min')
-    target_path = os.path.join(out_dir, filename_ext)
-    _clean_dir(out_dir, ext)
-
-    # save geneData
-    geneData.to_csv(target_path, sep='\t', index=False)
-    print('Minimised file saved at: %s' % target_path)
-
-    return target_path
-
-
-def _clean_dir(out_dir, ext):
-    if not os.path.exists(out_dir):
-        os.makedirs(out_dir)
-    else:
-        files = glob.glob(out_dir + '/*.'+ext)
-        for f in files:
-            print('removed file %s' % f)
-            os.remove(f)
-
-
-def _get_file(OUT_DIR, n, header_line):
-    filename = os.path.basename(OUT_DIR).split('.')[0]
-    file = os.path.join(OUT_DIR, filename + '_%d.%s' % (n, 'tsv'))
-    handle = open(file, "a", newline='', encoding='utf-8')
-    write = csv.writer(handle, delimiter='\t')
-    write.writerow(header_line)
-    return file, handle
-
-
-def _order_prob(df, n, class_name=[], prob=[]):
-    '''
-    orders the list that keeps the cell classes probs from highest to lowest.
-    Rearranges then the cell class names appropriately
-    :param df:
-    :param n:
-    :param class_name:
-    :param prob:
-    :return:
-    '''
-    for index, row in df.iterrows():
-        cn = np.array(json.loads(row['ClassName'].replace("'", '"')))
-        p = np.array(json.loads(row['Prob']))
-
-        idx = np.argsort(p)[::-1]
-        cn = cn[idx]
-        cn = cn.tolist()
-        p = p[idx]
-        class_name.append(cn[:n]) # keep the top-n only and append
-        prob.append(p[:n])
-    return [class_name, prob]
-
-
-def rotate_image(img_in, img_out, deg):
-    """
-    rotates an image.
-    img_in: path to the image to be rotated
-    img_out: path to save the rotated image to
-    deg: degrees to rotate the image by (clockwise)
-    """
-    x = pyvips.Image.new_from_file(img_in)
-    x = x.rotate(deg, interpolate=pyvips.Interpolate.new("nearest"))
-    x.write_to_file(img_out, compression="jpeg", tile=True)
-
-
-_format = lambda x: round(x, 3) # keep only 3 decimal points
-
-
-def _round_data(df, name):
-    neighbour_prob = [json.loads(x) for x in df[name]]
-    return [list(map(_format, x)) for x in neighbour_prob]
-
-
-def _round_data2(df, name):
-    return [list(map(_format, x)) for x in df[name]]
-
-
-
-if __name__ == "__main__":
-    mb_size = 99
-    n = 4
-    filepaths = [r"geneData.tsv"]
-    for filepath in filepaths:
-        splitter_mb(filepath, mb_size)
-        splitter_n(filepath, n)
```

## pciSeq/static/2D/viewer/js/classConfig.js

### js-beautify {}

```diff
@@ -307,15 +307,15 @@
         {
             className: 'Generic',
             IdentifiedType: 'Generic',
             color: '#C0C0C0'
         },
 
         // The donut chart at the bottom right will aggregate all classes with prob < 2% under the Other
-        // label (see line 101, donut.js)
+        // label (see line 278, donut.js)
         {
             className: 'Other',
             IdentifiedType: 'Other',
             color: '#C0C0C0'
         },
```

## pciSeq/static/2D/viewer/js/glyphConfig.js

### js-beautify {}

```diff
@@ -1,12 +1,17 @@
 // Set here the color scheme and the marker(glyph) shape for the gene panel you are working
-// with. These come into action at the deep zoom levels, when Leaflet.js takes over.
+// with. These gene-specific shapes come into action at the deep zoom levels, when Leaflet.js
+// takes over.
 //
 // There is switch for that called zoomSwitch, see line 39 of index.js together with the moveend
 // callback, see lines 459 and 511 of dapi.js.
+//
+// Outside this deep zoom zone all genes have the same shape, a solid circle, see
+// line 11, stage_markers_patched.js
+//
 // Note that leaflet doesnt get on very well with lots and lots of datapoints, if you notice
 // some slowness on the viewer maybe changing the zoomSwitch from its default value of 7 to 8
 // will probably help.
 //
 // Only the following shapes are supported:
 //   star6
 //   star5
@@ -18,15 +23,16 @@
 //   triangleLeft
 //   cross
 //   plus
 //   asterisk
 //   circle
 //   point
 //
-// These are designed in glyphPaths.js and introduced to the viewer from glyphs.js
+// These are designed in glyphPaths.js and introduced to the viewer from glyphs.js (no need to edit them
+// unless you want to do an extra shape).
 
 
 function glyphSettings() {
     var out = [
 
         {
             gene: 'Snca',
```

## pciSeq/static/2D/viewer/js/stage_markers_patched.js

### js-beautify {}

```diff
@@ -57,19 +57,15 @@
         //                     z === 5 ?  (2**z)/7602 :
         //                         z === 6 ? 0.03 * 2**1 : // every time you zoom in, leaflet scales up by 2. Divide here by 2 to keep the marker the same as in zoom level 5
         //                             z === 7 ? 0.03 * 2**0 :
         //                                 z === 8 ? 0.03 : (2**z)/7602
     }
 
     function scaleRampHelper(z, scale) {
-        // makes a tiny dot and the its scales it up based on the map and the dapi dimensions
-        // As a general remark also, keep in mind that every time you zoom in, leaflet (I think) scales up by 2.
-        // Divide by 2 to keep the marker the same as size. Hence if for zoom level = 3 the  return value from
-        // this function is lets say zo 10, then when to keep the same size on the screen for the dot, at zoom = 4
-        // the return value should be 5
+        // makes a tiny dot and then it scales it up based on the map and the dapi dimensions
         var map_side = mapSide(configSettings.zoomLevels),
             dapi_size = [configSettings.roi.x1 - configSettings.roi.x0, configSettings.roi.y1 - configSettings.roi.y0],
             max_dapi = Math.max(...dapi_size),
             c = map_side / max_dapi,
             tiny_dot = 1 / (2 ** z),
             dot = c * tiny_dot;
         return dot * scale
```

## Comparing `pciSeq-0.0.49.dist-info/LICENCE` & `pciSeq-0.0.50.dist-info/LICENCE`

 * *Files identical despite different names*

## Comparing `pciSeq-0.0.49.dist-info/METADATA` & `pciSeq-0.0.50.dist-info/METADATA`

 * *Files 11% similar despite different names*

```diff
@@ -1,87 +1,85 @@
-Metadata-Version: 2.1
-Name: pciSeq
-Version: 0.0.49
-Summary: Probabilistic cell typing for spatial transcriptomics
-Home-page: https://github.com/acycliq/pciSeq
-Author: Dimitris Nicoloutsopoulos
-Author-email: dimitris.nicoloutsopoulos@gmail.com
-License: BSD
-Classifier: Programming Language :: Python :: 3
-Classifier: License :: OSI Approved :: BSD License
-Classifier: Operating System :: OS Independent
-Description-Content-Type: text/markdown
-License-File: LICENCE
-Requires-Dist: opencv-python
-Requires-Dist: numpy-groupies
-Requires-Dist: pandas
-Requires-Dist: scipy
-Requires-Dist: scikit-image
-Requires-Dist: scikit-learn
-Requires-Dist: tqdm
-Requires-Dist: flask
-Requires-Dist: numexpr
-Requires-Dist: diplib
-Requires-Dist: pyvips
-Provides-Extra: interactive
-Requires-Dist: matplotlib (>=2.2.0) ; extra == 'interactive'
-Requires-Dist: jupyter ; extra == 'interactive'
-
-# pciSeq: Probabilistic Cell typing by In situ Sequencing
-A Python package that implements the cell calling algorithm as described in [Qian, X., et al. Nature Methods (2020)](https://www.nature.com/articles/s41592-019-0631-4)
-<p align="center">
-    <img src="viewer/assets/screencast_resized.gif" alt="screenshot"/>
-</p>
-
-## Installation
-```
-python -m pip install pciSeq
-```
-Requirement: Python >= 3.8
-
-If you want to work with the source code you can download the repo and then replicate the python environment by
-```
-conda env create -n pciSeq -f /path/to/environment.yml
-```
-
-That will create a conda environment with the name `pciSeq` containing all the necessary packages to run the algorithm. To activate it run 
-```
-conda activate pciSeq
-```
-or, if you open the project in your IDE, then in your project settings, switch your interpreter to the interpreter of the `pciSeq` env. 
-## Usage
-You need to create two `pandas dataframes` for the spots and the single cell data and a `coo_matrix` for the label image (which in 
-most cases will be the output of some image segmentation application). Then you pass them into the `pciSeq.fit()` method as follows: 
-```
-import pciSeq
-
-res = pciSeq.fit(spots_df, label_image, scRNA_df)
-```
-See the demo below for a more detailed explanation about the arguments of  `pciSeq.fit()` and its return values.
-
-There is also a fourth argument (optional) to override the default hyperparameter values which are initialised 
-by the [config.py](https://github.com/acycliq/pciSeq/blob/master/pciSeq/config.py) module. To pass user-defined hyperparameter values, create a `dictionary` with `keys` the
-hyperparameter names and `values` their new values. For example, to exclude all Npy and Vip spots you can do:
-
-```
-import pciSeq
-
-opts = { 'exclude_genes': ['Npy', 'Vip'] }
-res = pciSeq.fit(spots_df, label_image, scRNA_df, opts)
-```
-
-## Demo
-You can run a pciSeq demo in google colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/acycliq/pciSeq/blob/master/notebooks/1_pciSeq.ipynb)
-
-## Viewer
-An interactive viewer to explore the data runs on this [url](https://acycliq.github.io/visage/). Instructions about 
-building this viewer with your own data are [here](https://github.com/acycliq/visage). 
-
-If you have `pciSeq 0.0.48` or greater you also can launch the viewer automatically by 
-setting `{'launch_viewer': True}` and passing it to `pciSeq.fit()`, see [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/acycliq/pciSeq/blob/master/notebooks/2_viewer.ipynb)
-
-
-## References 
-Qian, X., et al. (2020). Probabilistic cell typing enables fine mapping of closely related cell types in situ. Nat
-Methods 17, 101 - 106.
-
-
+Metadata-Version: 2.1
+Name: pciSeq
+Version: 0.0.50
+Summary: Probabilistic cell typing for spatial transcriptomics
+Home-page: https://github.com/acycliq/pciSeq
+Author: Dimitris Nicoloutsopoulos
+Author-email: dimitris.nicoloutsopoulos@gmail.com
+License: BSD
+Classifier: Programming Language :: Python :: 3
+Classifier: License :: OSI Approved :: BSD License
+Classifier: Operating System :: OS Independent
+Description-Content-Type: text/markdown
+License-File: LICENCE
+Requires-Dist: numpy-groupies
+Requires-Dist: pandas
+Requires-Dist: scipy
+Requires-Dist: scikit-image
+Requires-Dist: scikit-learn
+Requires-Dist: tqdm
+Requires-Dist: flask
+Requires-Dist: numexpr
+Requires-Dist: diplib
+Requires-Dist: pyvips
+Provides-Extra: interactive
+Requires-Dist: matplotlib (>=2.2.0) ; extra == 'interactive'
+Requires-Dist: jupyter ; extra == 'interactive'
+
+# pciSeq: Probabilistic Cell typing by In situ Sequencing
+A Python package that implements the cell calling algorithm as described in [Qian, X., et al. Nature Methods (2020)](https://www.nature.com/articles/s41592-019-0631-4)
+<p align="center">
+    <img src="https://github.com/acycliq/pciSeq/blob/master/assets/screencast_resized.gif?raw=true" alt="screenshot"/>
+</p>
+
+## Installation
+```
+python -m pip install pciSeq
+```
+Requirement: Python >= 3.8
+
+If you want to work with the source code you can download the repo and then replicate the python environment by
+```
+conda env create -n pciSeq -f /path/to/environment.yml
+```
+
+That will create a conda environment with the name `pciSeq` containing all the necessary packages to run the algorithm. To activate it run 
+```
+conda activate pciSeq
+```
+or, if you open the project in your IDE, then in your project settings, switch your interpreter to the interpreter of the `pciSeq` env. 
+## Usage
+You need to create two `pandas dataframes` for the spots and the single cell data and a `coo_matrix` for the label image (which in 
+most cases will be the output of some image segmentation application). Then you pass them into the `pciSeq.fit()` method as follows: 
+```
+import pciSeq
+
+res = pciSeq.fit(spots_df, label_image, scRNA_df)
+```
+See the demo below for a more detailed explanation about the arguments of  `pciSeq.fit()` and its return values.
+
+There is also a fourth argument (optional) to override the default hyperparameter values which are initialised 
+by the [config.py](https://github.com/acycliq/pciSeq/blob/master/pciSeq/config.py) module. To pass user-defined hyperparameter values, create a `dictionary` with `keys` the
+hyperparameter names and `values` their new values. For example, to exclude all Npy and Vip spots you can do:
+
+```
+import pciSeq
+
+opts = { 'exclude_genes': ['Npy', 'Vip'] }
+res = pciSeq.fit(spots_df, label_image, scRNAseq=scRNA_df, opts=opts)
+```
+
+## Demo
+You can run a pciSeq demo in google colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/acycliq/pciSeq/blob/master/notebooks/1_pciSeq.ipynb)
+
+## Viewer
+An interactive viewer to explore the data runs on this [url](https://acycliq.github.io/visage/). Instructions about 
+building this viewer with your own data are [here](https://github.com/acycliq/visage). \
+If you have `v 0.0.49` or greater you can also launch the viewer automatically by 
+setting `opts = {'launch_viewer': True}` and passing it to `pciSeq.fit()`, see [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/acycliq/pciSeq/blob/master/notebooks/2_viewer.ipynb)
+
+
+## References 
+Qian, X., et al. (2020). Probabilistic cell typing enables fine mapping of closely related cell types in situ. Nat
+Methods 17, 101 - 106.
+
+
```

## Comparing `pciSeq-0.0.49.dist-info/RECORD` & `pciSeq-0.0.50.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -1,48 +1,48 @@
 pciSeq/__init__.py,sha256=fTmu6nn7JN8jNECjrJBPpSdFJFz2BBO8et5igUincVY,2086
-pciSeq/app.py,sha256=6jHV_-Au1HHrAA50r1gcXgTiDMgOwUaH-UW96_R8Oww,10388
+pciSeq/app.py,sha256=DGmK8PXbNxJGsRIpl9Czx6cgXpMaaguP3-YyUe2GfOM,9326
 pciSeq/config.py,sha256=M7iR2OAbptE5UldUa6t2cCiR9cyDwFEaTQbsKEe7yYY,3333
 pciSeq/make_plot.py,sha256=2AkncrVVvytr2keK505oMC0LV1Txli9WpsSGpjmht_4,3678
 pciSeq/src/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pciSeq/src/_version.py,sha256=77SS3zB_47rJdQjK2Dehjc2cDUl4tvHlUGBHvbgIang,22
+pciSeq/src/_version.py,sha256=5EpyNnUNHdR6H711I3BZSKvH8qRHZCfaQSj21Qkn1t4,22
 pciSeq/src/cell_call/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pciSeq/src/cell_call/datatypes.py,sha256=yHxAzzTqU2F_E2dtMNGglQFjAEdkGrtH8Ik8-flIVZ0,16311
+pciSeq/src/cell_call/datatypes.py,sha256=cZi8Wsr3KIkpQOiJXsKmxXuUFChPbyQGOu6AnNOS8CU,20581
 pciSeq/src/cell_call/log_config.py,sha256=vnm9TxlMEVG_GLPfl21GDEWTafEk3B-drTLsVQMfFEE,2995
-pciSeq/src/cell_call/main.py,sha256=f_FhOvpkgw-Ba3YV56IHZ1CeNohWJLVW2MBlNr2Rdss,10436
-pciSeq/src/cell_call/summary.py,sha256=TQwPGjtxshWcDvPTfnin7EcDfiF0aXrk5QUK2p3qmhU,3110
-pciSeq/src/cell_call/utils.py,sha256=V9qQwgIkTZoTWbJx0lyw56exizdA9oueGLFEPKq9mPE,8939
+pciSeq/src/cell_call/main.py,sha256=q_YGxPsVgLQW4fhRC5To27k3_HPYR6wI0cz7198j-4o,13927
+pciSeq/src/cell_call/summary.py,sha256=f2snLqJIeywtYuHrJLM-hRvifgqRU2u2aCKE_JrZJPs,3104
+pciSeq/src/cell_call/utils.py,sha256=sIrlkwdUgbfH7MmpOLw-Po4mTjsEWwNHHciUArHPN0A,8937
 pciSeq/src/preprocess/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pciSeq/src/preprocess/cell_borders.py,sha256=8O_Zycqsg5_te_UbS1Nh6I8ozyGOgAAzmPx0qL9mhBk,10926
+pciSeq/src/preprocess/cell_borders.py,sha256=eTuFuexqGMFEV4upuTRInoaAi75NN46TIcvJ36TbZ4Q,11253
 pciSeq/src/preprocess/segmentation.py,sha256=9fc47EsxeFQnNtbJ0vm-RqATsBBMP73J5chdRFJocd4,6973
-pciSeq/src/preprocess/spot_labels.py,sha256=pLUs4gELxAEH1Q_uPaWOM0jXy1SXtMjh9hS8-lytRtA,4099
+pciSeq/src/preprocess/spot_labels.py,sha256=Y33S1wvhkgMKk9YjIOJZTDX5OyHpJDieM2exW0IrDB0,4090
 pciSeq/src/preprocess/utils.py,sha256=7UB-fOKsSY_a4MVPs7E6BoNZcwITqm2N2JFMZqNYbj0,88
 pciSeq/src/viewer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pciSeq/src/viewer/run_flask.py,sha256=H-GcMVOKr2WJhlCnOmLlQgL5UpEWswkoGweyMfHwpgc,2442
+pciSeq/src/viewer/run_flask.py,sha256=9HS_RW-O9xddAeiApd4m_w2JJmbqUGZ5lhQ-z_HUeVA,2256
 pciSeq/src/viewer/stage_image.py,sha256=ksrCt0LklOehdUdXTENFH3kvhSafRAtHAVHd22lSQQM,4511
-pciSeq/src/viewer/utils.py,sha256=7ZfXm9_vwsdXKJy4BfpzbujMcX-CmPMIH5dPz4lLluo,8464
+pciSeq/src/viewer/utils.py,sha256=8TtwWtoqox6Cy0otCIs6d10xwWUcEQgBkgVhG3HWVOg,7638
 pciSeq/static/2D/index.html,sha256=9aMR5hOaXugJpSOt3ypUY24z6blsfOS83Cd9LHbAHa0,10895
 pciSeq/static/2D/viewer/genes_datatable.html,sha256=v10lTjkXYeAv0DVuaWHOB6XgG887k-_ABXAgobDV-tY,6506
 pciSeq/static/2D/viewer/css/index.css,sha256=IntuFc7kNAaNkLSL3m_9fpnH67i5KJpNWJ0OIW4TmD4,9622
 pciSeq/static/2D/viewer/css/progress.css,sha256=0MfjX9kIwD4wBeC39LBi9bCvu4ZkHwg_vtbuUiy4CuI,3135
-pciSeq/static/2D/viewer/js/classConfig.js,sha256=5tpLfKJX1EC0I5UWlUAcHvofv5oBRgoa4gUy38eYP90,7435
+pciSeq/static/2D/viewer/js/classConfig.js,sha256=xhfWJHcU1bXipgJEokkMdUsrPDifPHpp_UHzW2qYopg,7435
 pciSeq/static/2D/viewer/js/config.js,sha256=m8t0igFi2m0r-RtyF8QqgMm_97z0ptvy2x88JrKrNVQ,1211
 pciSeq/static/2D/viewer/js/customControl.js,sha256=niQd9FT0z_Dkp7v523n8plj7kNRnRpvADcHOChbDskY,5589
 pciSeq/static/2D/viewer/js/dapi.js,sha256=hZwgaEhIfMARoBStZoz3bMyO8PetD9NJufboq3flN-A,22840
 pciSeq/static/2D/viewer/js/dataLoader.js,sha256=iD3Kv9XNsSD2YyU3ilau5iaxAEQEX5VJnQ06VWyEP-g,5984
 pciSeq/static/2D/viewer/js/donut.js,sha256=dn9S3ZnNTFdEaiDU7ZadoKjVyvYp9LEJm82EvCe1ySY,17232
 pciSeq/static/2D/viewer/js/dt.js,sha256=eJpyKHNJjs1jf-GiTEa2_UsxMadO5eRWs2J0sJMhFms,4106
-pciSeq/static/2D/viewer/js/glyphConfig.js,sha256=eVo6tEWogtiAYFKq6-8BXjiLC_borKENcrPDpTRu5DI,9616
+pciSeq/static/2D/viewer/js/glyphConfig.js,sha256=23YX0DGc7XQXuJrUJDt59lref_TZPwUGF_Pr4h8c-tk,9835
 pciSeq/static/2D/viewer/js/glyphPaths.js,sha256=M3gwhPIRR17Q6cokLU754uovl5VwHQciduJKLlPhUXY,4389
 pciSeq/static/2D/viewer/js/glyphs.js,sha256=m6VFmUxNDJju9fXG9VzOKOnxip2mGbaBOyQKOY5_6zg,7224
 pciSeq/static/2D/viewer/js/index.js,sha256=IFMrGHYnC0fKzDgTfVlTrvLAqdNxlkjv_3_WNSAc7hw,12281
 pciSeq/static/2D/viewer/js/progress.js,sha256=781S0wMmKngJZxM7-opHPd3duQqlrdALLZZAwwACak8,1073
 pciSeq/static/2D/viewer/js/stage_cells.js,sha256=2rGYqLgUoMuvcI443YOPd9UiDTl85NP3GCQEUMEByPo,6367
 pciSeq/static/2D/viewer/js/stage_glyphs.js,sha256=egi2-UvAhO4aQ8vSOza_ld2opaJ2tclzqYqHneRnmmI,12064
 pciSeq/static/2D/viewer/js/stage_markers.js,sha256=nkEhmtk-8VGQbabDGK0fl4FURst9uUnKAGJcVGM9hcw,9540
-pciSeq/static/2D/viewer/js/stage_markers_patched.js,sha256=yeRttCtB6roX2__Jq4AX8JtNBAobENTtcXVyBrVIASo,7533
+pciSeq/static/2D/viewer/js/stage_markers_patched.js,sha256=ujIExH9uuve5DjcMZP2G8C_7yoBvEFegEsJFZHDt-Wo,7150
 pciSeq/static/2D/viewer/js/stage_polygons.js,sha256=G0Jxc7ocU2pNLKYtS-14_9jpGrFIR6RDiOyrOZ35PLI,15523
 pciSeq/static/2D/viewer/js/streaming-tsv-parser.js,sha256=ENPcWuYtmkUSDArHJ_1BnpnfJuFq-kpcOJSidE1I-Dw,4471
 pciSeq/static/2D/viewer/js/viewerUtils.js,sha256=609GVJUZY0P5wPTiPrgOZjeG_cmvm2jM7xze9Nl5H_0,4692
 pciSeq/static/2D/viewer/js/lib/css/L.Control.Layers.Tree.css,sha256=8M2Z28r2e7tj4G0eN52nlqGVunxHxMWHequekzF-fpo,995
 pciSeq/static/2D/viewer/js/lib/css/Leaflet.Coordinates-0.1.3.css,sha256=W6ZGjm-QrMQy_6C6xfkPA8tgk_Oh6JY3StLhQUrK_fA,421
 pciSeq/static/2D/viewer/js/lib/css/bootstrap.min.css,sha256=fO2Fh9Otx1Ft-Cy6-PgzCTeWj4fR-yJ7G9BrYgQNM9k,122544
 pciSeq/static/2D/viewer/js/lib/css/index.css,sha256=Tz2XH-M7Yg87oHCXT9T1v4WtfmVfZbxCKDgUMaTNgDU,8669
@@ -55,13 +55,13 @@
 pciSeq/static/2D/viewer/js/lib/js/leaflet-pip.js,sha256=QqsilU0SHTaKA_qNSv_jEDH53cWO6T_RdforfybnbKM,15374
 pciSeq/static/2D/viewer/js/lib/js/leaflet.textpath.js,sha256=i5cmMPqpqXShZz0TvXU3ZQUi2Y07SIbIpMYbhPE8gWE,5927
 pciSeq/static/2D/viewer/js/lib/js/preloader.js,sha256=eaFWRf9yT-Td4C2t01P8fDEZYszYTaGDgQ1vjevNIDI,265
 pciSeq/static/2D/viewer/js/lib/js/pixiOverlay/MarkerContainer.js,sha256=Z8TADCqYEQr25OEF7wljsOVAOpx87UrjFPI2nvIty70,9097
 pciSeq/static/2D/viewer/js/lib/js/pixiOverlay/bezier-easing.js,sha256=yMoRDqJiglg0M0eWM25WGA5zzLWuJpOyC3a8aGFx0jw,4434
 pciSeq/static/2D/viewer/js/lib/js/pixiOverlay/example.min.js,sha256=bZEnK8XSnShrpLYtOS7Xq7wnp-nW_nMZh4aaDc1Gt6U,613808
 pciSeq/static/2D/viewer/js/lib/js/pixiOverlay/tools.min.js,sha256=A027Ub-oWXvKbz2B3pgMFS5xApmnEHXz9GQh79YZZnU,91874
-pciSeq-0.0.49.dist-info/LICENCE,sha256=6kbiFSfobTZ7beWiKnHpN902HgBx-Jzgcme0SvKqhKY,1091
-pciSeq-0.0.49.dist-info/METADATA,sha256=nb0sT6AUJ1xkhahZMeepN1cpkwDTvisQRw4XRfXG64A,3749
-pciSeq-0.0.49.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-pciSeq-0.0.49.dist-info/entry_points.txt,sha256=RGOkAd0S7rrTuEInj-jKu5B7630i4R3mK9v9vx6nqc8,48
-pciSeq-0.0.49.dist-info/top_level.txt,sha256=mSonMN0SaB51m5yLGm5hFE1Fthf-2U4sRKxSezY-JMk,7
-pciSeq-0.0.49.dist-info/RECORD,,
+pciSeq-0.0.50.dist-info/LICENCE,sha256=6kbiFSfobTZ7beWiKnHpN902HgBx-Jzgcme0SvKqhKY,1091
+pciSeq-0.0.50.dist-info/METADATA,sha256=ZqX9Ba36SW7QYEtrZIvs93d0jDoq5f9IdyA6q_U7VIM,3697
+pciSeq-0.0.50.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+pciSeq-0.0.50.dist-info/entry_points.txt,sha256=RGOkAd0S7rrTuEInj-jKu5B7630i4R3mK9v9vx6nqc8,48
+pciSeq-0.0.50.dist-info/top_level.txt,sha256=mSonMN0SaB51m5yLGm5hFE1Fthf-2U4sRKxSezY-JMk,7
+pciSeq-0.0.50.dist-info/RECORD,,
```

